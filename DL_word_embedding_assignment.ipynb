{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7IoUq5E1eJeI"
   },
   "source": [
    "# Word Embedding - Home Assigment\n",
    "## Dr. Omri Allouche 2018. YData Deep Learning Course\n",
    "\n",
    "[Open in Google Colab](https://colab.research.google.com/github/omriallouche/deep_learning_course/blob/master/DL_word_embedding_assignment.ipynb)\n",
    "    \n",
    "    \n",
    "In this exercise, you'll use word vectors trained on a corpus of 380,000 lyrics of songs from MetroLyrics (https://www.kaggle.com/gyani95/380000-lyrics-from-metrolyrics).  \n",
    "The dataset contains these fields for each song, in CSV format:\n",
    "1. index\n",
    "1. song\n",
    "1. year\n",
    "1. artist\n",
    "1. genre\n",
    "1. lyrics\n",
    "\n",
    "Before doing this exercise, we recommend that you go over the \"Bag of words meets bag of popcorn\" tutorial (https://www.kaggle.com/c/word2vec-nlp-tutorial)\n",
    "\n",
    "Other recommended resources:\n",
    "- https://rare-technologies.com/word2vec-tutorial/\n",
    "- https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jv13T-wMoMEm"
   },
   "source": [
    "First we'll download the data from Kaggle. For that let's follow the following [Stackoveflow answer](https://stackoverflow.com/questions/49310470/using-kaggle-datasets-in-google-colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7522,
     "status": "ok",
     "timestamp": 1571208711665,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "TIIewzdBnns1",
    "outputId": "f46d0188-e92c-4888-f0c8-63f2a953b9c6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-a3916b97-400f-420d-927e-e7d8173ee7dc\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-a3916b97-400f-420d-927e-e7d8173ee7dc\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving kaggle.json to kaggle.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'kaggle.json': b'{\"username\":\"asafam\",\"key\":\"c4ea37f8767c3430e4d17c7cda57fd48\"}\\n'}"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's run this cell and select the kaggle.json file downloaded\n",
    "# from the Kaggle account settings page.\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1821,
     "status": "ok",
     "timestamp": 1571208719278,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "mzuZ5AUEogIZ",
    "outputId": "8434f18b-ffe1-4702-efd2-aeb3a38fdd16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 63 Oct 16 06:51 kaggle.json\n"
     ]
    }
   ],
   "source": [
    "# Let's make sure the kaggle.json file is present.\n",
    "!ls -lha kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bX1NaLBLom4s"
   },
   "outputs": [],
   "source": [
    "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
    "# so move it there.\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "\n",
    "# This permissions change avoids a warning on Kaggle tool startup.\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z9T48i0QoASu"
   },
   "outputs": [],
   "source": [
    "# Next, install the Kaggle API client.\n",
    "!pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5022,
     "status": "ok",
     "timestamp": 1571208730862,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "v2rgcisQo6Xr",
    "outputId": "02a8d0f9-7bf5-47ea-89d7-18ae4be8ac7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref                                                       title                                               size  lastUpdated          downloadCount  voteCount  usabilityRating  \r\n",
      "--------------------------------------------------------  -------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \r\n",
      "tristan581/17k-apple-app-store-strategy-games             17K Mobile Strategy Games                            8MB  2019-08-26 08:22:16           3175         79  1.0              \r\n",
      "dgomonov/new-york-city-airbnb-open-data                   New York City Airbnb Open Data                       2MB  2019-08-12 16:24:45          33529        817  1.0              \r\n",
      "gustavomodelli/forest-fires-in-brazil                     Forest Fires in Brazil                              31KB  2019-08-24 16:09:16           4375         95  0.9411765        \r\n",
      "shuyangli94/food-com-recipes-and-user-interactions        Food.com Recipes and Interactions                  261MB  2019-10-12 06:30:37           1395         44  1.0              \r\n",
      "rajeevw/ufcdata                                           UFC-Fight historical data from 1993 to 2019          2MB  2019-07-05 09:58:02           2780         74  0.9705882        \r\n",
      "chirin/africa-economic-banking-and-systemic-crisis-data   Africa Economic, Banking and Systemic Crisis Data   14KB  2019-07-21 02:00:17           1523         36  1.0              \r\n",
      "bradklassen/pga-tour-20102018-data                        PGA Tour Golf Data                                 102MB  2019-10-15 03:14:40           8146        302  1.0              \r\n",
      "lakshyaag/india-trade-data                                India - Trade Data                                   1MB  2019-08-16 16:13:58          13654        285  1.0              \r\n",
      "akhilv11/border-crossing-entry-data                       Border Crossing Entry Data                           3MB  2019-08-21 14:51:34           1477         38  1.0              \r\n",
      "ruslankl/european-union-lgbt-survey-2012                  EU LGBT Survey                                     577KB  2019-07-19 11:15:25            782         30  1.0              \r\n",
      "ma7555/schengen-visa-stats                                Schengen Visa Stats 2017/2018                        1MB  2019-07-25 10:55:37           2437         66  0.9705882        \r\n",
      "dareenalharthi/jamalon-arabic-books-dataset               Jamalon Arabic Books Dataset                         1MB  2019-08-15 18:58:06            803         33  1.0              \r\n",
      "codersree/mount-rainier-weather-and-climbing-data         Mount Rainier Weather and Climbing Data             25KB  2019-08-27 23:33:36           3026         78  0.9705882        \r\n",
      "kapilverma/hindi-bible                                    Hindi Bible                                          4MB  2019-09-07 18:04:35            127          5  1.0              \r\n",
      "samhiatt/xenocanto-avian-vocalizations-canv-usa           Avian Vocalizations from CA & NV, USA                1GB  2019-08-10 00:16:10            612         12  1.0              \r\n",
      "therohk/ireland-historical-news                           The Irish Times - Waxy-Wany News                    47MB  2019-08-24 15:36:54           2463        138  0.625            \r\n",
      "lishuyangkaggle/cocktails-hotaling-co                     Cocktails (Hotaling & Co.)                          75KB  2019-07-08 23:49:34           2317         55  1.0              \r\n",
      "martj42/international-football-results-from-1872-to-2017  International football results from 1872 to 2019   525KB  2019-10-02 16:51:16          25034        736  0.9411765        \r\n",
      "hmavrodiev/sofia-air-quality-dataset                      Sofia air quality dataset                            3GB  2019-09-14 05:48:09            494         11  0.882352948      \r\n",
      "valentynsichkar/traffic-signs-preprocessed                Traffic Signs Preprocessed                            0B  2019-08-31 18:22:11            443         23  1.0              \r\n"
     ]
    }
   ],
   "source": [
    "# List available datasets.\n",
    "!kaggle datasets list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3748,
     "status": "ok",
     "timestamp": 1571208735298,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "_Mmn_j0-pEAA",
    "outputId": "361621c2-8e41-473e-b009-414af44964a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 380000-lyrics-from-metrolyrics.zip to /Users/asaf/Workspace/python/biu-advanced-deep-learning\n",
      "100%|██████████████████████████████████████| 95.6M/95.6M [00:11<00:00, 8.90MB/s]\n",
      "100%|██████████████████████████████████████| 95.6M/95.6M [00:11<00:00, 8.94MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Copy the MetroLyrics data set locally.\n",
    "!kaggle datasets download -d gyani95/380000-lyrics-from-metrolyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1928,
     "status": "ok",
     "timestamp": 1571208751657,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "dZUEH7fdsTBv",
    "outputId": "99729dc7-9f03-4eac-e4ac-b46c64c67f77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300features_40minwords_10context\r\n",
      "380000-lyrics-from-metrolyrics.zip\r\n",
      "DL_rnn_text_classification_generation.ipynb\r\n",
      "DL_word_embedding_assignment.ipynb\r\n",
      "README.md\r\n",
      "environment.yml\r\n"
     ]
    }
   ],
   "source": [
    "!ls ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 78
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5607,
     "status": "ok",
     "timestamp": 1571130837997,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "yU-DJKgqsd9Y",
    "outputId": "4a482ee8-b78c-4955-b38c-9058b4f37800"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  380000-lyrics-from-metrolyrics.zip\n",
      "replace lyrics.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "!unzip 380000-lyrics-from-metrolyrics.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xdsGSCf9sik0"
   },
   "outputs": [],
   "source": [
    "!head lyrics.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YEJedZwVeJeJ"
   },
   "source": [
    "## Train word vectors\n",
    "Train word vectors using the Skipgram Word2vec algorithm and the gensim package.\n",
    "Make sure you perform the following:\n",
    "- Tokenize words\n",
    "- Lowercase all words\n",
    "- Remove punctuation marks\n",
    "- Remove rare words\n",
    "- Remove stopwords\n",
    "\n",
    "Use 300 as the dimension of the word vectors. Try different context sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1478,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9E6Y-DfxeJeK"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1479,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZG6fCgKZmYEL"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "raw_data = pd.read_csv(\"380000-lyrics-from-metrolyrics.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wBZNvcxauE2c"
   },
   "source": [
    "Let's see the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1480,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 520,
     "status": "ok",
     "timestamp": 1571208773345,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "HbTbGYKQtd3A",
    "outputId": "68c388a7-b90c-4e6f-fd32-43205f031178"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['index', 'song', 'year', 'artist', 'genre', 'lyrics'], dtype=object)"
      ]
     },
     "execution_count": 1480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1482,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 365,
     "status": "ok",
     "timestamp": 1571208773574,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "3G6TLFDFtf_Y",
    "outputId": "637f9096-e782-4aad-eef7-83e96ba1033e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(362237, 6)"
      ]
     },
     "execution_count": 1482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1483,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 476,
     "status": "ok",
     "timestamp": 1571208774013,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "NP1NPzU_tnKe",
    "outputId": "6635f803-a920-4dcd-dd00-fa5838c0f791"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index                    song  year           artist genre  \\\n",
      "0      0               ego-remix  2009  beyonce-knowles   Pop   \n",
      "1      1            then-tell-me  2009  beyonce-knowles   Pop   \n",
      "2      2                 honesty  2009  beyonce-knowles   Pop   \n",
      "3      3         you-are-my-rock  2009  beyonce-knowles   Pop   \n",
      "4      4           black-culture  2009  beyonce-knowles   Pop   \n",
      "5      5  all-i-could-do-was-cry  2009  beyonce-knowles   Pop   \n",
      "6      6      once-in-a-lifetime  2009  beyonce-knowles   Pop   \n",
      "7      7                 waiting  2009  beyonce-knowles   Pop   \n",
      "8      8               slow-love  2009  beyonce-knowles   Pop   \n",
      "9      9   why-don-t-you-love-me  2009  beyonce-knowles   Pop   \n",
      "\n",
      "                                              lyrics  \n",
      "0  Oh baby, how you doing?\\nYou know I'm gonna cu...  \n",
      "1  playin' everything so easy,\\nit's like you see...  \n",
      "2  If you search\\nFor tenderness\\nIt isn't hard t...  \n",
      "3  Oh oh oh I, oh oh oh I\\n[Verse 1:]\\nIf I wrote...  \n",
      "4  Party the people, the people the party it's po...  \n",
      "5  I heard\\nChurch bells ringing\\nI heard\\nA choi...  \n",
      "6  This is just another day that I would spend\\nW...  \n",
      "7  Waiting, waiting, waiting, waiting\\nWaiting, w...  \n",
      "8  [Verse 1:]\\nI read all of the magazines\\nwhile...  \n",
      "9  N-n-now, honey\\nYou better sit down and look a...  \n"
     ]
    }
   ],
   "source": [
    "print(raw_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rock          131377\n",
       "Pop            49444\n",
       "Hip-Hop        33965\n",
       "Metal          28408\n",
       "Other          23683\n",
       "Country        17286\n",
       "Jazz           17147\n",
       "Electronic     16205\n",
       "R&B             5935\n",
       "Indie           5732\n",
       "Folk            3241\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 1493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1498,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 855,
     "status": "ok",
     "timestamp": 1571208775209,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "KehiITwuf-eM",
    "outputId": "fbb8e2fc-1a63-4a27-9fca-4d68a24738d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(332423, 6)\n",
      "(266505, 6)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "data = raw_data.loc[raw_data[\"lyrics\"].str.len() > 3]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1499,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1947,
     "status": "ok",
     "timestamp": 1571208778915,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "Mz8ixq82Hywk",
    "outputId": "407a27d7-0f52-47a4-95ce-156e2ba6b507"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/asaf/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1500,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NfTH8vy90VFK"
   },
   "outputs": [],
   "source": [
    "# Import various modules for string cleaning\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def document_to_words(text, remove_panctuations=True, remove_stopwords=False):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove panctuation marks\n",
    "    if remove_panctuations:\n",
    "      text = re.sub(\"[^a-zA-Z0-9]\",\" \", text)\n",
    "    #\n",
    "    # 2. Convert words to lower case and split them\n",
    "    words = text.lower().split()\n",
    "    #\n",
    "    # 3. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w.lower() in stops]   \n",
    "    #\n",
    "    # 4. Return a list of words\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1501,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 380,
     "status": "ok",
     "timestamp": 1571208779196,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "JVErO0ZCY8qF",
    "outputId": "3f2c4625-5f0c-4bbf-d72d-774492196f3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh',\n",
       " 'baby',\n",
       " 'know',\n",
       " 'gonna',\n",
       " 'cut',\n",
       " 'right',\n",
       " 'chase',\n",
       " 'women',\n",
       " 'made',\n",
       " 'like']"
      ]
     },
     "execution_count": 1501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_to_words(data[\"lyrics\"][0], remove_stopwords=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1502,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 978,
     "status": "ok",
     "timestamp": 1571208780824,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "bVQ6FFuhSzC4",
    "outputId": "862622d7-69af-4929-9305-8ad0e48bcead"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/asaf/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1503,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 547,
     "status": "ok",
     "timestamp": 1571208781452,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "n8VeP3Q0XhkP",
    "outputId": "b7e0db51-c480-45e9-bf28-cfd39e36a4d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Oh baby, how you doing?',\n",
       " \"You know I'm gonna cut right to the chase\\nSome women were made but me, myself\\nI like to think that I was created for a special purpose\\nYou know, what's more special than you?\",\n",
       " \"You feel me\\nIt's on baby, let's get lost\\nYou don't need to call into work 'cause you're the boss\\nFor real, want you to show me how you feel\\nI consider myself lucky, that's a big deal\\nWhy?\",\n",
       " \"Well, you got the key to my heart\\nBut you ain't gonna need it, I'd rather you open up my body\\nAnd show me secrets, you didn't know was inside\\nNo need for me to lie\\nIt's too big, it's too wide\\nIt's too strong, it won't fit\\nIt's too much, it's too tough\\nHe talk like this 'cause he can back it up\\nHe got a big ego, such a huge ego\\nI love his big ego, it's too much\\nHe walk like this 'cause he can back it up\\nUsually I'm humble, right now I don't choose\\nYou can leave with me or you could have the blues\\nSome call it arrogant, I call it confident\\nYou decide when you find on what I'm working with\\nDamn I know I'm killing you with them legs\\nBetter yet them thighs\\nMatter a fact it's my smile or maybe my eyes\\nBoy you a site to see, kind of something like me\\nIt's too big, it's too wide\\nIt's too strong, it won't fit\\nIt's too much, it's too tough\\nI talk like this 'cause I can back it up\\nI got a big ego, such a huge ego\\nBut he love my big ego, it's too much\\nI walk like this 'cause I can back it up\\nI, I walk like this 'cause I can back it up\\nI, I talk like this 'cause I can back it up\\nI, I can back it up, I can back it up\\nI walk like this 'cause I can back it up\\nIt's too big, it's too wide\\nIt's too strong, it won't fit\\nIt's too much, it's too tough\\nHe talk like this 'cause he can back it up\\nHe got a big ego, such a huge ego, such a huge ego\\nI love his big ego, it's too much\\nHe walk like this 'cause he can back it up\\nEgo so big, you must admit\\nI got every reason to feel like I'm that bitch\\nEgo so strong, if you ain't know\\nI don't need no beat, I can sing it with piano\"]"
      ]
     },
     "execution_count": 1503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(data[\"lyrics\"][0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1504,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_KFVuoZz1HCP"
   },
   "outputs": [],
   "source": [
    "# Define a function to split a document into parsed sentences\n",
    "def document_to_sentences(document, tokenizer, remove_stopwords=False):\n",
    "    # Function to split a document into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(document) if tokenizer else document\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(document_to_words(raw_sentence, remove_stopwords))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1505,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 495,
     "status": "ok",
     "timestamp": 1571208783499,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "iXRza46XkVlY",
    "outputId": "a39f13aa-42af-423e-a65c-01df27961e54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 1505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = document_to_sentences(data[\"lyrics\"][0], tokenizer, remove_stopwords=True)\n",
    "len(sentence[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1506,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 74454,
     "status": "ok",
     "timestamp": 1571208858450,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "ehhwUhqXWP3v",
    "outputId": "33f00bbc-2470-4056-d928-9cd14860b589"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "for song_lyrics in data[\"lyrics\"]:\n",
    "    sentences += document_to_sentences(song_lyrics, tokenizer, \n",
    "                                       remove_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1507,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 73258,
     "status": "ok",
     "timestamp": 1571208858451,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "95uszIi9kB5a",
    "outputId": "19d7c2b5-7b5e-4bc2-92d8-38ba99deb4ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['oh', 'baby', 'how', 'you', 'doing'], ['you', 'know', 'i', 'm', 'gonna', 'cut', 'right', 'to', 'the', 'chase', 'some', 'women', 'were', 'made', 'but', 'me', 'myself', 'i', 'like', 'to', 'think', 'that', 'i', 'was', 'created', 'for', 'a', 'special', 'purpose', 'you', 'know', 'what', 's', 'more', 'special', 'than', 'you'], ['you', 'feel', 'me', 'it', 's', 'on', 'baby', 'let', 's', 'get', 'lost', 'you', 'don', 't', 'need', 'to', 'call', 'into', 'work', 'cause', 'you', 're', 'the', 'boss', 'for', 'real', 'want', 'you', 'to', 'show', 'me', 'how', 'you', 'feel', 'i', 'consider', 'myself', 'lucky', 'that', 's', 'a', 'big', 'deal', 'why']]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1509,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 69900,
     "status": "ok",
     "timestamp": 1571208858673,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "zJ6A1Ge_TeEm",
    "outputId": "e079be1e-f229-4cf0-d66e-ae446b8f54f2"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def word2vec(sentences, num_features=300, min_word_count=40, workers=4, \n",
    "             context=10, downsampling=1e-3, save_model=True):\n",
    "\n",
    "  # Initialize and train the model (this will take some time)\n",
    "  print(\"Training model...\")\n",
    "  model = Word2Vec(sentences,\n",
    "                   workers=workers,\n",
    "                   size=num_features, \n",
    "                   min_count=min_word_count,\n",
    "                   window=context, \n",
    "                   sample=downsampling)\n",
    "\n",
    "  if save_model:\n",
    "    # If you don't plan to train the model any further, calling \n",
    "    # init_sims will make the model much more memory-efficient.\n",
    "    model.init_sims(replace=True)\n",
    "\n",
    "    # It can be helpful to create a meaningful model name and \n",
    "    # save the model for later use. You can load it later using Word2Vec.load()\n",
    "    model_name = \"{}features_{}minwords_{}context.wv.model\".format(num_features, \n",
    "                                                          min_word_count, \n",
    "                                                          context)\n",
    "    model.save(model_name)\n",
    " \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1510,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 379328,
     "status": "ok",
     "timestamp": 1571209279279,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "0Tuj4cl3bt68",
    "outputId": "23aa3627-ca66-4b3d-8609-564f94bcbe45"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:17:22: collecting all words and their counts\n",
      "INFO - 21:17:22: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:17:22: PROGRESS: at sentence #10000, processed 547073 words, keeping 24856 word types\n",
      "INFO - 21:17:22: PROGRESS: at sentence #20000, processed 1002175 words, keeping 36701 word types\n",
      "INFO - 21:17:23: PROGRESS: at sentence #30000, processed 1577546 words, keeping 45437 word types\n",
      "INFO - 21:17:23: PROGRESS: at sentence #40000, processed 2132629 words, keeping 55005 word types\n",
      "INFO - 21:17:23: PROGRESS: at sentence #50000, processed 2565209 words, keeping 60035 word types\n",
      "INFO - 21:17:23: PROGRESS: at sentence #60000, processed 3125605 words, keeping 70378 word types\n",
      "INFO - 21:17:23: PROGRESS: at sentence #70000, processed 3650087 words, keeping 80685 word types\n",
      "INFO - 21:17:24: PROGRESS: at sentence #80000, processed 4152681 words, keeping 88085 word types\n",
      "INFO - 21:17:24: PROGRESS: at sentence #90000, processed 4712834 words, keeping 92527 word types\n",
      "INFO - 21:17:24: PROGRESS: at sentence #100000, processed 5346594 words, keeping 95618 word types\n",
      "INFO - 21:17:24: PROGRESS: at sentence #110000, processed 5804237 words, keeping 97869 word types\n",
      "INFO - 21:17:24: PROGRESS: at sentence #120000, processed 6238748 words, keeping 101497 word types\n",
      "INFO - 21:17:25: PROGRESS: at sentence #130000, processed 6747822 words, keeping 106167 word types\n",
      "INFO - 21:17:25: PROGRESS: at sentence #140000, processed 7264439 words, keeping 110304 word types\n",
      "INFO - 21:17:25: PROGRESS: at sentence #150000, processed 7826998 words, keeping 113139 word types\n",
      "INFO - 21:17:25: PROGRESS: at sentence #160000, processed 8341793 words, keeping 119257 word types\n",
      "INFO - 21:17:25: PROGRESS: at sentence #170000, processed 8873575 words, keeping 123630 word types\n",
      "INFO - 21:17:26: PROGRESS: at sentence #180000, processed 9412714 words, keeping 129259 word types\n",
      "INFO - 21:17:26: PROGRESS: at sentence #190000, processed 9966143 words, keeping 135923 word types\n",
      "INFO - 21:17:26: PROGRESS: at sentence #200000, processed 10526414 words, keeping 141990 word types\n",
      "INFO - 21:17:26: PROGRESS: at sentence #210000, processed 10982261 words, keeping 144912 word types\n",
      "INFO - 21:17:26: PROGRESS: at sentence #220000, processed 11367169 words, keeping 149330 word types\n",
      "INFO - 21:17:26: PROGRESS: at sentence #230000, processed 11988166 words, keeping 153274 word types\n",
      "INFO - 21:17:27: PROGRESS: at sentence #240000, processed 12532308 words, keeping 156680 word types\n",
      "INFO - 21:17:27: PROGRESS: at sentence #250000, processed 13079502 words, keeping 159781 word types\n",
      "INFO - 21:17:27: PROGRESS: at sentence #260000, processed 13567570 words, keeping 162857 word types\n",
      "INFO - 21:17:27: PROGRESS: at sentence #270000, processed 14076921 words, keeping 168418 word types\n",
      "INFO - 21:17:27: PROGRESS: at sentence #280000, processed 14645191 words, keeping 171879 word types\n",
      "INFO - 21:17:28: PROGRESS: at sentence #290000, processed 15200162 words, keeping 177206 word types\n",
      "INFO - 21:17:28: PROGRESS: at sentence #300000, processed 15647288 words, keeping 179877 word types\n",
      "INFO - 21:17:28: PROGRESS: at sentence #310000, processed 16214258 words, keeping 182211 word types\n",
      "INFO - 21:17:28: PROGRESS: at sentence #320000, processed 16785917 words, keeping 186105 word types\n",
      "INFO - 21:17:28: PROGRESS: at sentence #330000, processed 17218338 words, keeping 188173 word types\n",
      "INFO - 21:17:28: PROGRESS: at sentence #340000, processed 17774404 words, keeping 192031 word types\n",
      "INFO - 21:17:29: PROGRESS: at sentence #350000, processed 18370845 words, keeping 195242 word types\n",
      "INFO - 21:17:29: PROGRESS: at sentence #360000, processed 18946568 words, keeping 197477 word types\n",
      "INFO - 21:17:29: PROGRESS: at sentence #370000, processed 19440294 words, keeping 201296 word types\n",
      "INFO - 21:17:29: PROGRESS: at sentence #380000, processed 19925594 words, keeping 202681 word types\n",
      "INFO - 21:17:29: PROGRESS: at sentence #390000, processed 20452587 words, keeping 208062 word types\n",
      "INFO - 21:17:30: PROGRESS: at sentence #400000, processed 20844516 words, keeping 211371 word types\n",
      "INFO - 21:17:30: PROGRESS: at sentence #410000, processed 21428945 words, keeping 214812 word types\n",
      "INFO - 21:17:30: PROGRESS: at sentence #420000, processed 21881705 words, keeping 219208 word types\n",
      "INFO - 21:17:30: PROGRESS: at sentence #430000, processed 22462819 words, keeping 220763 word types\n",
      "INFO - 21:17:30: PROGRESS: at sentence #440000, processed 23066330 words, keeping 223281 word types\n",
      "INFO - 21:17:31: PROGRESS: at sentence #450000, processed 23662350 words, keeping 225996 word types\n",
      "INFO - 21:17:31: PROGRESS: at sentence #460000, processed 24224611 words, keeping 232691 word types\n",
      "INFO - 21:17:31: PROGRESS: at sentence #470000, processed 24849515 words, keeping 236393 word types\n",
      "INFO - 21:17:31: PROGRESS: at sentence #480000, processed 25402691 words, keeping 239509 word types\n",
      "INFO - 21:17:31: PROGRESS: at sentence #490000, processed 25896125 words, keeping 242861 word types\n",
      "INFO - 21:17:32: PROGRESS: at sentence #500000, processed 26428231 words, keeping 244691 word types\n",
      "INFO - 21:17:32: PROGRESS: at sentence #510000, processed 26955817 words, keeping 247883 word types\n",
      "INFO - 21:17:32: PROGRESS: at sentence #520000, processed 27487614 words, keeping 249512 word types\n",
      "INFO - 21:17:32: PROGRESS: at sentence #530000, processed 27971469 words, keeping 251533 word types\n",
      "INFO - 21:17:32: PROGRESS: at sentence #540000, processed 28490015 words, keeping 252916 word types\n",
      "INFO - 21:17:32: PROGRESS: at sentence #550000, processed 28989811 words, keeping 254959 word types\n",
      "INFO - 21:17:33: PROGRESS: at sentence #560000, processed 29475702 words, keeping 257380 word types\n",
      "INFO - 21:17:33: PROGRESS: at sentence #570000, processed 29924960 words, keeping 259980 word types\n",
      "INFO - 21:17:33: PROGRESS: at sentence #580000, processed 30435339 words, keeping 262416 word types\n",
      "INFO - 21:17:33: PROGRESS: at sentence #590000, processed 30986320 words, keeping 264861 word types\n",
      "INFO - 21:17:33: PROGRESS: at sentence #600000, processed 31561787 words, keeping 268688 word types\n",
      "INFO - 21:17:34: PROGRESS: at sentence #610000, processed 32112877 words, keeping 272019 word types\n",
      "INFO - 21:17:34: PROGRESS: at sentence #620000, processed 32642375 words, keeping 274986 word types\n",
      "INFO - 21:17:34: PROGRESS: at sentence #630000, processed 33062014 words, keeping 277041 word types\n",
      "INFO - 21:17:34: PROGRESS: at sentence #640000, processed 33543969 words, keeping 279806 word types\n",
      "INFO - 21:17:34: PROGRESS: at sentence #650000, processed 34230688 words, keeping 282203 word types\n",
      "INFO - 21:17:35: PROGRESS: at sentence #660000, processed 34791272 words, keeping 285091 word types\n",
      "INFO - 21:17:35: PROGRESS: at sentence #670000, processed 35309627 words, keeping 286699 word types\n",
      "INFO - 21:17:35: PROGRESS: at sentence #680000, processed 35812309 words, keeping 287788 word types\n",
      "INFO - 21:17:35: PROGRESS: at sentence #690000, processed 36402616 words, keeping 290155 word types\n",
      "INFO - 21:17:35: PROGRESS: at sentence #700000, processed 36931058 words, keeping 292733 word types\n",
      "INFO - 21:17:36: PROGRESS: at sentence #710000, processed 37459344 words, keeping 294911 word types\n",
      "INFO - 21:17:36: PROGRESS: at sentence #720000, processed 38035701 words, keeping 298075 word types\n",
      "INFO - 21:17:36: PROGRESS: at sentence #730000, processed 38526517 words, keeping 300419 word types\n",
      "INFO - 21:17:36: PROGRESS: at sentence #740000, processed 39078186 words, keeping 301908 word types\n",
      "INFO - 21:17:36: PROGRESS: at sentence #750000, processed 39602798 words, keeping 304326 word types\n",
      "INFO - 21:17:37: PROGRESS: at sentence #760000, processed 40204170 words, keeping 306727 word types\n",
      "INFO - 21:17:37: PROGRESS: at sentence #770000, processed 40727304 words, keeping 308543 word types\n",
      "INFO - 21:17:37: PROGRESS: at sentence #780000, processed 41308108 words, keeping 311788 word types\n",
      "INFO - 21:17:37: PROGRESS: at sentence #790000, processed 41840703 words, keeping 314452 word types\n",
      "INFO - 21:17:37: PROGRESS: at sentence #800000, processed 42396409 words, keeping 317538 word types\n",
      "INFO - 21:17:38: PROGRESS: at sentence #810000, processed 42881628 words, keeping 321897 word types\n",
      "INFO - 21:17:38: PROGRESS: at sentence #820000, processed 43471094 words, keeping 323912 word types\n",
      "INFO - 21:17:38: PROGRESS: at sentence #830000, processed 44172564 words, keeping 325593 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:17:38: PROGRESS: at sentence #840000, processed 44667279 words, keeping 330357 word types\n",
      "INFO - 21:17:38: PROGRESS: at sentence #850000, processed 45114992 words, keeping 331606 word types\n",
      "INFO - 21:17:39: PROGRESS: at sentence #860000, processed 45840735 words, keeping 334894 word types\n",
      "INFO - 21:17:39: PROGRESS: at sentence #870000, processed 46364336 words, keeping 338759 word types\n",
      "INFO - 21:17:39: PROGRESS: at sentence #880000, processed 46858701 words, keeping 340849 word types\n",
      "INFO - 21:17:39: PROGRESS: at sentence #890000, processed 47371857 words, keeping 345358 word types\n",
      "INFO - 21:17:40: PROGRESS: at sentence #900000, processed 47954498 words, keeping 346951 word types\n",
      "INFO - 21:17:40: PROGRESS: at sentence #910000, processed 48459183 words, keeping 351406 word types\n",
      "INFO - 21:17:40: PROGRESS: at sentence #920000, processed 48891710 words, keeping 353783 word types\n",
      "INFO - 21:17:40: PROGRESS: at sentence #930000, processed 49400367 words, keeping 355945 word types\n",
      "INFO - 21:17:40: PROGRESS: at sentence #940000, processed 49930820 words, keeping 357697 word types\n",
      "INFO - 21:17:41: PROGRESS: at sentence #950000, processed 50289438 words, keeping 358285 word types\n",
      "INFO - 21:17:41: PROGRESS: at sentence #960000, processed 50841972 words, keeping 359892 word types\n",
      "INFO - 21:17:41: PROGRESS: at sentence #970000, processed 51449504 words, keeping 362113 word types\n",
      "INFO - 21:17:41: PROGRESS: at sentence #980000, processed 51944940 words, keeping 365262 word types\n",
      "INFO - 21:17:41: PROGRESS: at sentence #990000, processed 52419089 words, keeping 367630 word types\n",
      "INFO - 21:17:42: PROGRESS: at sentence #1000000, processed 52917528 words, keeping 369143 word types\n",
      "INFO - 21:17:42: PROGRESS: at sentence #1010000, processed 53450021 words, keeping 371849 word types\n",
      "INFO - 21:17:42: PROGRESS: at sentence #1020000, processed 54024823 words, keeping 374148 word types\n",
      "INFO - 21:17:42: PROGRESS: at sentence #1030000, processed 54713585 words, keeping 375960 word types\n",
      "INFO - 21:17:43: PROGRESS: at sentence #1040000, processed 55288244 words, keeping 377553 word types\n",
      "INFO - 21:17:43: PROGRESS: at sentence #1050000, processed 55911724 words, keeping 379082 word types\n",
      "INFO - 21:17:43: PROGRESS: at sentence #1060000, processed 56549332 words, keeping 382291 word types\n",
      "INFO - 21:17:43: PROGRESS: at sentence #1070000, processed 56868533 words, keeping 383411 word types\n",
      "INFO - 21:17:43: PROGRESS: at sentence #1080000, processed 57449247 words, keeping 387694 word types\n",
      "INFO - 21:17:44: PROGRESS: at sentence #1090000, processed 58059497 words, keeping 389822 word types\n",
      "INFO - 21:17:44: PROGRESS: at sentence #1100000, processed 58470622 words, keeping 393227 word types\n",
      "INFO - 21:17:44: PROGRESS: at sentence #1110000, processed 58993729 words, keeping 395024 word types\n",
      "INFO - 21:17:44: PROGRESS: at sentence #1120000, processed 59292418 words, keeping 396371 word types\n",
      "INFO - 21:17:44: PROGRESS: at sentence #1130000, processed 59812235 words, keeping 398692 word types\n",
      "INFO - 21:17:44: PROGRESS: at sentence #1140000, processed 60462312 words, keeping 402023 word types\n",
      "INFO - 21:17:45: PROGRESS: at sentence #1150000, processed 60957239 words, keeping 404330 word types\n",
      "INFO - 21:17:45: PROGRESS: at sentence #1160000, processed 61457197 words, keeping 405916 word types\n",
      "INFO - 21:17:45: PROGRESS: at sentence #1170000, processed 62011934 words, keeping 408061 word types\n",
      "INFO - 21:17:45: PROGRESS: at sentence #1180000, processed 62484493 words, keeping 410107 word types\n",
      "INFO - 21:17:45: PROGRESS: at sentence #1190000, processed 63056200 words, keeping 412233 word types\n",
      "INFO - 21:17:46: collected 414720 word types from a corpus of 63696185 raw words and 1199770 sentences\n",
      "INFO - 21:17:46: Loading a fresh vocabulary\n",
      "INFO - 21:17:46: effective_min_count=40 retains 29972 unique words (7% of original 414720, drops 384748)\n",
      "INFO - 21:17:46: effective_min_count=40 leaves 62171067 word corpus (97% of original 63696185, drops 1525118)\n",
      "INFO - 21:17:46: deleting the raw counts dictionary of 414720 items\n",
      "INFO - 21:17:46: sample=0.001 downsamples 61 most-common words\n",
      "INFO - 21:17:46: downsampling leaves estimated 45784945 word corpus (73.6% of prior 62171067)\n",
      "INFO - 21:17:46: estimated required memory for 29972 words and 300 dimensions: 86918800 bytes\n",
      "INFO - 21:17:46: resetting layer weights\n",
      "INFO - 21:17:54: training model with 4 workers on 29972 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "INFO - 21:17:55: EPOCH 1 - PROGRESS: at 1.72% examples, 751645 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:17:56: EPOCH 1 - PROGRESS: at 3.46% examples, 779607 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:17:57: EPOCH 1 - PROGRESS: at 5.49% examples, 820965 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:17:58: EPOCH 1 - PROGRESS: at 7.48% examples, 839859 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:17:59: EPOCH 1 - PROGRESS: at 9.37% examples, 843557 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:00: EPOCH 1 - PROGRESS: at 11.37% examples, 850009 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:01: EPOCH 1 - PROGRESS: at 13.21% examples, 846570 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:02: EPOCH 1 - PROGRESS: at 15.15% examples, 848606 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:03: EPOCH 1 - PROGRESS: at 17.08% examples, 852364 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:04: EPOCH 1 - PROGRESS: at 19.17% examples, 854655 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:05: EPOCH 1 - PROGRESS: at 21.12% examples, 856469 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:06: EPOCH 1 - PROGRESS: at 22.74% examples, 845820 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:07: EPOCH 1 - PROGRESS: at 24.66% examples, 844126 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:08: EPOCH 1 - PROGRESS: at 26.44% examples, 845427 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:09: EPOCH 1 - PROGRESS: at 28.36% examples, 846434 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:10: EPOCH 1 - PROGRESS: at 30.08% examples, 847275 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:11: EPOCH 1 - PROGRESS: at 31.99% examples, 848310 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:12: EPOCH 1 - PROGRESS: at 34.14% examples, 849933 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:13: EPOCH 1 - PROGRESS: at 36.03% examples, 850785 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:14: EPOCH 1 - PROGRESS: at 37.71% examples, 851194 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:15: EPOCH 1 - PROGRESS: at 39.39% examples, 851578 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:16: EPOCH 1 - PROGRESS: at 41.24% examples, 850803 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:17: EPOCH 1 - PROGRESS: at 43.07% examples, 849285 words/s, in_qsize 8, out_qsize 0\n",
      "INFO - 21:18:18: EPOCH 1 - PROGRESS: at 45.08% examples, 849443 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:19: EPOCH 1 - PROGRESS: at 46.79% examples, 844820 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:20: EPOCH 1 - PROGRESS: at 48.58% examples, 841842 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:21: EPOCH 1 - PROGRESS: at 50.04% examples, 836654 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:22: EPOCH 1 - PROGRESS: at 51.65% examples, 833693 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:23: EPOCH 1 - PROGRESS: at 53.54% examples, 832818 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 21:18:24: EPOCH 1 - PROGRESS: at 55.18% examples, 832245 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:25: EPOCH 1 - PROGRESS: at 56.84% examples, 828843 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:26: EPOCH 1 - PROGRESS: at 58.63% examples, 828467 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:27: EPOCH 1 - PROGRESS: at 60.38% examples, 829118 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:28: EPOCH 1 - PROGRESS: at 62.14% examples, 829808 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:29: EPOCH 1 - PROGRESS: at 63.94% examples, 830845 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:30: EPOCH 1 - PROGRESS: at 65.80% examples, 830990 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:31: EPOCH 1 - PROGRESS: at 67.68% examples, 831416 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:32: EPOCH 1 - PROGRESS: at 69.16% examples, 831571 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:33: EPOCH 1 - PROGRESS: at 71.11% examples, 831925 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:18:34: EPOCH 1 - PROGRESS: at 72.88% examples, 832401 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:35: EPOCH 1 - PROGRESS: at 74.68% examples, 832533 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:36: EPOCH 1 - PROGRESS: at 76.71% examples, 833078 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:37: EPOCH 1 - PROGRESS: at 78.78% examples, 833347 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:38: EPOCH 1 - PROGRESS: at 80.73% examples, 833896 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:39: EPOCH 1 - PROGRESS: at 82.67% examples, 834646 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:40: EPOCH 1 - PROGRESS: at 84.62% examples, 835207 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:41: EPOCH 1 - PROGRESS: at 86.16% examples, 835475 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:42: EPOCH 1 - PROGRESS: at 87.77% examples, 835687 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:43: EPOCH 1 - PROGRESS: at 89.86% examples, 835845 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:44: EPOCH 1 - PROGRESS: at 91.77% examples, 836790 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:45: EPOCH 1 - PROGRESS: at 94.09% examples, 837339 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:47: EPOCH 1 - PROGRESS: at 95.76% examples, 837450 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:48: EPOCH 1 - PROGRESS: at 97.78% examples, 838261 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:49: EPOCH 1 - PROGRESS: at 99.61% examples, 838770 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:49: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:18:49: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:18:49: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:18:49: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:18:49: EPOCH - 1 : training on 63696185 raw words (45783640 effective words) took 54.6s, 838957 effective words/s\n",
      "INFO - 21:18:50: EPOCH 2 - PROGRESS: at 1.96% examples, 864125 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:51: EPOCH 2 - PROGRESS: at 3.91% examples, 868499 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:52: EPOCH 2 - PROGRESS: at 5.82% examples, 866865 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:53: EPOCH 2 - PROGRESS: at 7.71% examples, 872954 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:54: EPOCH 2 - PROGRESS: at 9.77% examples, 870682 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:55: EPOCH 2 - PROGRESS: at 11.72% examples, 871613 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:56: EPOCH 2 - PROGRESS: at 13.77% examples, 874334 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:57: EPOCH 2 - PROGRESS: at 15.58% examples, 874294 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:58: EPOCH 2 - PROGRESS: at 17.31% examples, 862436 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:18:59: EPOCH 2 - PROGRESS: at 19.30% examples, 863022 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:00: EPOCH 2 - PROGRESS: at 21.27% examples, 863660 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:01: EPOCH 2 - PROGRESS: at 23.18% examples, 864311 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:02: EPOCH 2 - PROGRESS: at 25.10% examples, 863630 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:03: EPOCH 2 - PROGRESS: at 26.92% examples, 864353 words/s, in_qsize 8, out_qsize 0\n",
      "INFO - 21:19:04: EPOCH 2 - PROGRESS: at 28.87% examples, 865168 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:05: EPOCH 2 - PROGRESS: at 30.75% examples, 865150 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 21:19:06: EPOCH 2 - PROGRESS: at 32.99% examples, 867497 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:07: EPOCH 2 - PROGRESS: at 34.84% examples, 866694 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:08: EPOCH 2 - PROGRESS: at 36.68% examples, 867555 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:09: EPOCH 2 - PROGRESS: at 38.44% examples, 868117 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:10: EPOCH 2 - PROGRESS: at 40.18% examples, 868239 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:11: EPOCH 2 - PROGRESS: at 42.31% examples, 868986 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:12: EPOCH 2 - PROGRESS: at 44.14% examples, 867892 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:13: EPOCH 2 - PROGRESS: at 46.25% examples, 868619 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:14: EPOCH 2 - PROGRESS: at 48.36% examples, 869935 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:15: EPOCH 2 - PROGRESS: at 50.21% examples, 870582 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:16: EPOCH 2 - PROGRESS: at 52.18% examples, 870983 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:17: EPOCH 2 - PROGRESS: at 54.05% examples, 870517 words/s, in_qsize 8, out_qsize 0\n",
      "INFO - 21:19:18: EPOCH 2 - PROGRESS: at 55.93% examples, 871096 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:19: EPOCH 2 - PROGRESS: at 57.89% examples, 871489 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:20: EPOCH 2 - PROGRESS: at 59.74% examples, 871513 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:21: EPOCH 2 - PROGRESS: at 61.67% examples, 871692 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:22: EPOCH 2 - PROGRESS: at 63.48% examples, 871240 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:23: EPOCH 2 - PROGRESS: at 65.28% examples, 870925 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:24: EPOCH 2 - PROGRESS: at 67.18% examples, 871332 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:25: EPOCH 2 - PROGRESS: at 68.78% examples, 871049 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:26: EPOCH 2 - PROGRESS: at 70.87% examples, 871378 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:27: EPOCH 2 - PROGRESS: at 72.46% examples, 870649 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:28: EPOCH 2 - PROGRESS: at 74.05% examples, 866474 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:29: EPOCH 2 - PROGRESS: at 75.70% examples, 864303 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:30: EPOCH 2 - PROGRESS: at 77.66% examples, 862578 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:31: EPOCH 2 - PROGRESS: at 79.84% examples, 862385 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:32: EPOCH 2 - PROGRESS: at 81.69% examples, 862694 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:33: EPOCH 2 - PROGRESS: at 83.53% examples, 860490 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:34: EPOCH 2 - PROGRESS: at 84.99% examples, 856829 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:35: EPOCH 2 - PROGRESS: at 86.37% examples, 854484 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:36: EPOCH 2 - PROGRESS: at 87.96% examples, 854624 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:37: EPOCH 2 - PROGRESS: at 90.01% examples, 854475 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:38: EPOCH 2 - PROGRESS: at 91.93% examples, 854828 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:39: EPOCH 2 - PROGRESS: at 94.26% examples, 855009 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:40: EPOCH 2 - PROGRESS: at 95.96% examples, 854830 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:41: EPOCH 2 - PROGRESS: at 97.78% examples, 853937 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:42: EPOCH 2 - PROGRESS: at 99.32% examples, 851195 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:43: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:19:43: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:19:43: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:19:43: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:19:43: EPOCH - 2 : training on 63696185 raw words (45786746 effective words) took 53.9s, 850165 effective words/s\n",
      "INFO - 21:19:44: EPOCH 3 - PROGRESS: at 1.72% examples, 749211 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:45: EPOCH 3 - PROGRESS: at 2.96% examples, 671451 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:46: EPOCH 3 - PROGRESS: at 4.40% examples, 652708 words/s, in_qsize 8, out_qsize 0\n",
      "INFO - 21:19:47: EPOCH 3 - PROGRESS: at 5.34% examples, 592101 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:48: EPOCH 3 - PROGRESS: at 6.39% examples, 566220 words/s, in_qsize 8, out_qsize 0\n",
      "INFO - 21:19:49: EPOCH 3 - PROGRESS: at 7.53% examples, 562504 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:50: EPOCH 3 - PROGRESS: at 8.41% examples, 547539 words/s, in_qsize 8, out_qsize 0\n",
      "INFO - 21:19:51: EPOCH 3 - PROGRESS: at 9.68% examples, 538056 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:19:52: EPOCH 3 - PROGRESS: at 10.71% examples, 529976 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:53: EPOCH 3 - PROGRESS: at 11.74% examples, 523302 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:54: EPOCH 3 - PROGRESS: at 12.90% examples, 521698 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:55: EPOCH 3 - PROGRESS: at 14.35% examples, 534500 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:56: EPOCH 3 - PROGRESS: at 15.85% examples, 547196 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:57: EPOCH 3 - PROGRESS: at 17.62% examples, 561164 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:58: EPOCH 3 - PROGRESS: at 19.42% examples, 577103 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:19:59: EPOCH 3 - PROGRESS: at 20.79% examples, 580333 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 21:20:00: EPOCH 3 - PROGRESS: at 22.31% examples, 586151 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:01: EPOCH 3 - PROGRESS: at 23.85% examples, 590216 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:02: EPOCH 3 - PROGRESS: at 25.54% examples, 598560 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:03: EPOCH 3 - PROGRESS: at 27.03% examples, 607025 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:04: EPOCH 3 - PROGRESS: at 28.96% examples, 618675 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:05: EPOCH 3 - PROGRESS: at 30.86% examples, 629557 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:06: EPOCH 3 - PROGRESS: at 32.65% examples, 637715 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:07: EPOCH 3 - PROGRESS: at 34.44% examples, 643520 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:08: EPOCH 3 - PROGRESS: at 35.79% examples, 640126 words/s, in_qsize 8, out_qsize 0\n",
      "INFO - 21:20:09: EPOCH 3 - PROGRESS: at 36.99% examples, 639793 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:10: EPOCH 3 - PROGRESS: at 38.38% examples, 641336 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:11: EPOCH 3 - PROGRESS: at 39.67% examples, 642370 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:12: EPOCH 3 - PROGRESS: at 41.23% examples, 644037 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:13: EPOCH 3 - PROGRESS: at 42.36% examples, 637978 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 21:20:14: EPOCH 3 - PROGRESS: at 43.23% examples, 630949 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:15: EPOCH 3 - PROGRESS: at 44.36% examples, 626185 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:16: EPOCH 3 - PROGRESS: at 45.43% examples, 620335 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 21:20:17: EPOCH 3 - PROGRESS: at 46.44% examples, 615173 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:18: EPOCH 3 - PROGRESS: at 47.79% examples, 612176 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:19: EPOCH 3 - PROGRESS: at 48.70% examples, 607087 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 21:20:20: EPOCH 3 - PROGRESS: at 49.60% examples, 602904 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 21:20:21: EPOCH 3 - PROGRESS: at 50.59% examples, 599111 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:22: EPOCH 3 - PROGRESS: at 51.70% examples, 597033 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:23: EPOCH 3 - PROGRESS: at 53.13% examples, 595427 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:24: EPOCH 3 - PROGRESS: at 54.00% examples, 593266 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 21:20:25: EPOCH 3 - PROGRESS: at 55.07% examples, 591020 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 21:20:26: EPOCH 3 - PROGRESS: at 56.58% examples, 592726 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:27: EPOCH 3 - PROGRESS: at 58.26% examples, 597559 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:28: EPOCH 3 - PROGRESS: at 59.95% examples, 602113 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:29: EPOCH 3 - PROGRESS: at 61.49% examples, 604741 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 21:20:30: EPOCH 3 - PROGRESS: at 63.25% examples, 609375 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:31: EPOCH 3 - PROGRESS: at 64.96% examples, 613254 words/s, in_qsize 8, out_qsize 0\n",
      "INFO - 21:20:32: EPOCH 3 - PROGRESS: at 66.63% examples, 616337 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:33: EPOCH 3 - PROGRESS: at 68.39% examples, 620700 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:34: EPOCH 3 - PROGRESS: at 69.82% examples, 623456 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:35: EPOCH 3 - PROGRESS: at 71.41% examples, 626069 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:36: EPOCH 3 - PROGRESS: at 73.18% examples, 629160 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:37: EPOCH 3 - PROGRESS: at 74.90% examples, 632859 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:38: EPOCH 3 - PROGRESS: at 76.99% examples, 636850 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:39: EPOCH 3 - PROGRESS: at 78.97% examples, 639781 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:40: EPOCH 3 - PROGRESS: at 80.68% examples, 641208 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:41: EPOCH 3 - PROGRESS: at 81.77% examples, 639471 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:42: EPOCH 3 - PROGRESS: at 83.07% examples, 638637 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:43: EPOCH 3 - PROGRESS: at 84.45% examples, 637725 words/s, in_qsize 8, out_qsize 0\n",
      "INFO - 21:20:44: EPOCH 3 - PROGRESS: at 85.59% examples, 636956 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:45: EPOCH 3 - PROGRESS: at 86.68% examples, 635974 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:46: EPOCH 3 - PROGRESS: at 87.79% examples, 635314 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:47: EPOCH 3 - PROGRESS: at 89.28% examples, 634633 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:48: EPOCH 3 - PROGRESS: at 90.51% examples, 634541 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:49: EPOCH 3 - PROGRESS: at 92.68% examples, 637832 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:50: EPOCH 3 - PROGRESS: at 94.74% examples, 641000 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:51: EPOCH 3 - PROGRESS: at 96.29% examples, 642618 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:52: EPOCH 3 - PROGRESS: at 97.84% examples, 643098 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 21:20:53: EPOCH 3 - PROGRESS: at 99.20% examples, 642747 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:54: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:20:54: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:20:54: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:20:54: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:20:54: EPOCH - 3 : training on 63696185 raw words (45785898 effective words) took 71.4s, 640854 effective words/s\n",
      "INFO - 21:20:55: EPOCH 4 - PROGRESS: at 0.95% examples, 437585 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 21:20:56: EPOCH 4 - PROGRESS: at 2.00% examples, 436058 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:57: EPOCH 4 - PROGRESS: at 2.99% examples, 449357 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:58: EPOCH 4 - PROGRESS: at 4.41% examples, 487389 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:20:59: EPOCH 4 - PROGRESS: at 5.68% examples, 506169 words/s, in_qsize 8, out_qsize 0\n",
      "INFO - 21:21:00: EPOCH 4 - PROGRESS: at 7.03% examples, 520808 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:01: EPOCH 4 - PROGRESS: at 8.11% examples, 524530 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:02: EPOCH 4 - PROGRESS: at 9.48% examples, 529930 words/s, in_qsize 8, out_qsize 0\n",
      "INFO - 21:21:03: EPOCH 4 - PROGRESS: at 10.84% examples, 533131 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:04: EPOCH 4 - PROGRESS: at 12.06% examples, 537541 words/s, in_qsize 8, out_qsize 0\n",
      "INFO - 21:21:05: EPOCH 4 - PROGRESS: at 13.48% examples, 543415 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:06: EPOCH 4 - PROGRESS: at 14.75% examples, 546396 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:07: EPOCH 4 - PROGRESS: at 15.86% examples, 546522 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:08: EPOCH 4 - PROGRESS: at 17.20% examples, 548750 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:09: EPOCH 4 - PROGRESS: at 18.66% examples, 548391 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:10: EPOCH 4 - PROGRESS: at 19.79% examples, 549596 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:11: EPOCH 4 - PROGRESS: at 21.48% examples, 561287 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:12: EPOCH 4 - PROGRESS: at 23.26% examples, 576490 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:13: EPOCH 4 - PROGRESS: at 25.20% examples, 590682 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:21:14: EPOCH 4 - PROGRESS: at 26.88% examples, 601873 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:15: EPOCH 4 - PROGRESS: at 28.78% examples, 613060 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:16: EPOCH 4 - PROGRESS: at 30.39% examples, 620013 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:17: EPOCH 4 - PROGRESS: at 31.90% examples, 622485 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:18: EPOCH 4 - PROGRESS: at 33.95% examples, 631769 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:19: EPOCH 4 - PROGRESS: at 35.84% examples, 640275 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:20: EPOCH 4 - PROGRESS: at 37.49% examples, 648368 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:21: EPOCH 4 - PROGRESS: at 39.15% examples, 655375 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:22: EPOCH 4 - PROGRESS: at 41.01% examples, 662610 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:23: EPOCH 4 - PROGRESS: at 42.94% examples, 669518 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:24: EPOCH 4 - PROGRESS: at 44.90% examples, 676081 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:25: EPOCH 4 - PROGRESS: at 47.03% examples, 682228 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:26: EPOCH 4 - PROGRESS: at 49.01% examples, 688395 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:27: EPOCH 4 - PROGRESS: at 50.71% examples, 692096 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:28: EPOCH 4 - PROGRESS: at 52.69% examples, 696237 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:29: EPOCH 4 - PROGRESS: at 54.32% examples, 698546 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:30: EPOCH 4 - PROGRESS: at 56.06% examples, 702400 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:31: EPOCH 4 - PROGRESS: at 57.82% examples, 705487 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:32: EPOCH 4 - PROGRESS: at 59.58% examples, 708147 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:33: EPOCH 4 - PROGRESS: at 61.39% examples, 711441 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:34: EPOCH 4 - PROGRESS: at 63.22% examples, 715253 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:35: EPOCH 4 - PROGRESS: at 64.88% examples, 716928 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:36: EPOCH 4 - PROGRESS: at 66.55% examples, 718184 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:37: EPOCH 4 - PROGRESS: at 68.32% examples, 720860 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:38: EPOCH 4 - PROGRESS: at 69.81% examples, 722347 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 21:21:39: EPOCH 4 - PROGRESS: at 71.56% examples, 725050 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:40: EPOCH 4 - PROGRESS: at 73.33% examples, 726523 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 21:21:41: EPOCH 4 - PROGRESS: at 75.16% examples, 729159 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:42: EPOCH 4 - PROGRESS: at 77.22% examples, 731533 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:43: EPOCH 4 - PROGRESS: at 79.35% examples, 733788 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:44: EPOCH 4 - PROGRESS: at 80.90% examples, 734332 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:45: EPOCH 4 - PROGRESS: at 82.89% examples, 736544 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:47: EPOCH 4 - PROGRESS: at 84.71% examples, 738245 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:48: EPOCH 4 - PROGRESS: at 86.25% examples, 739939 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:49: EPOCH 4 - PROGRESS: at 87.81% examples, 741633 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:50: EPOCH 4 - PROGRESS: at 89.87% examples, 743453 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:51: EPOCH 4 - PROGRESS: at 91.70% examples, 745036 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:52: EPOCH 4 - PROGRESS: at 93.88% examples, 745969 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:53: EPOCH 4 - PROGRESS: at 95.44% examples, 746796 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:54: EPOCH 4 - PROGRESS: at 97.24% examples, 748275 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:55: EPOCH 4 - PROGRESS: at 99.13% examples, 749532 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:55: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:21:55: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:21:55: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:21:55: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:21:55: EPOCH - 4 : training on 63696185 raw words (45785098 effective words) took 61.0s, 750665 effective words/s\n",
      "INFO - 21:21:56: EPOCH 5 - PROGRESS: at 1.95% examples, 845513 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:57: EPOCH 5 - PROGRESS: at 3.89% examples, 860970 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:58: EPOCH 5 - PROGRESS: at 5.82% examples, 863246 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:21:59: EPOCH 5 - PROGRESS: at 7.66% examples, 863208 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:00: EPOCH 5 - PROGRESS: at 9.48% examples, 850807 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:01: EPOCH 5 - PROGRESS: at 11.32% examples, 846925 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:02: EPOCH 5 - PROGRESS: at 13.29% examples, 848643 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:03: EPOCH 5 - PROGRESS: at 15.17% examples, 849428 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:04: EPOCH 5 - PROGRESS: at 17.14% examples, 851973 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:05: EPOCH 5 - PROGRESS: at 19.10% examples, 849332 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:06: EPOCH 5 - PROGRESS: at 20.93% examples, 850384 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:07: EPOCH 5 - PROGRESS: at 22.71% examples, 844289 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:08: EPOCH 5 - PROGRESS: at 24.44% examples, 836947 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:09: EPOCH 5 - PROGRESS: at 25.77% examples, 814288 words/s, in_qsize 7, out_qsize 1\n",
      "INFO - 21:22:10: EPOCH 5 - PROGRESS: at 27.02% examples, 802103 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:11: EPOCH 5 - PROGRESS: at 28.62% examples, 793565 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:12: EPOCH 5 - PROGRESS: at 30.22% examples, 791882 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:13: EPOCH 5 - PROGRESS: at 32.05% examples, 795009 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:14: EPOCH 5 - PROGRESS: at 34.16% examples, 798361 words/s, in_qsize 8, out_qsize 0\n",
      "INFO - 21:22:15: EPOCH 5 - PROGRESS: at 36.03% examples, 801133 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:16: EPOCH 5 - PROGRESS: at 37.62% examples, 801010 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:17: EPOCH 5 - PROGRESS: at 39.17% examples, 801041 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:18: EPOCH 5 - PROGRESS: at 40.67% examples, 796172 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:19: EPOCH 5 - PROGRESS: at 41.49% examples, 778849 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:20: EPOCH 5 - PROGRESS: at 43.26% examples, 778615 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:21: EPOCH 5 - PROGRESS: at 45.11% examples, 778194 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 21:22:22: EPOCH 5 - PROGRESS: at 46.96% examples, 778637 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:23: EPOCH 5 - PROGRESS: at 48.78% examples, 779764 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:24: EPOCH 5 - PROGRESS: at 50.51% examples, 780792 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:25: EPOCH 5 - PROGRESS: at 52.21% examples, 780813 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:26: EPOCH 5 - PROGRESS: at 53.87% examples, 780682 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:27: EPOCH 5 - PROGRESS: at 55.73% examples, 782163 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:28: EPOCH 5 - PROGRESS: at 57.44% examples, 783206 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:29: EPOCH 5 - PROGRESS: at 58.96% examples, 780451 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:30: EPOCH 5 - PROGRESS: at 60.70% examples, 781131 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:32: EPOCH 5 - PROGRESS: at 62.47% examples, 781540 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:33: EPOCH 5 - PROGRESS: at 63.96% examples, 781604 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:34: EPOCH 5 - PROGRESS: at 65.64% examples, 781573 words/s, in_qsize 8, out_qsize 0\n",
      "INFO - 21:22:35: EPOCH 5 - PROGRESS: at 67.34% examples, 780981 words/s, in_qsize 8, out_qsize 0\n",
      "INFO - 21:22:36: EPOCH 5 - PROGRESS: at 68.59% examples, 778221 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:22:37: EPOCH 5 - PROGRESS: at 70.23% examples, 777508 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:38: EPOCH 5 - PROGRESS: at 71.76% examples, 777903 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:39: EPOCH 5 - PROGRESS: at 73.37% examples, 776106 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:40: EPOCH 5 - PROGRESS: at 75.07% examples, 776337 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:41: EPOCH 5 - PROGRESS: at 76.37% examples, 771109 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:42: EPOCH 5 - PROGRESS: at 78.15% examples, 770526 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:43: EPOCH 5 - PROGRESS: at 79.88% examples, 768306 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 21:22:44: EPOCH 5 - PROGRESS: at 81.14% examples, 765403 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:45: EPOCH 5 - PROGRESS: at 82.84% examples, 764510 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:46: EPOCH 5 - PROGRESS: at 84.33% examples, 762571 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:47: EPOCH 5 - PROGRESS: at 85.57% examples, 760462 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:48: EPOCH 5 - PROGRESS: at 86.61% examples, 756564 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:49: EPOCH 5 - PROGRESS: at 88.01% examples, 756662 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:50: EPOCH 5 - PROGRESS: at 89.86% examples, 755602 words/s, in_qsize 8, out_qsize 2\n",
      "INFO - 21:22:51: EPOCH 5 - PROGRESS: at 91.28% examples, 754286 words/s, in_qsize 7, out_qsize 1\n",
      "INFO - 21:22:52: EPOCH 5 - PROGRESS: at 93.56% examples, 755523 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:53: EPOCH 5 - PROGRESS: at 95.18% examples, 756739 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:54: EPOCH 5 - PROGRESS: at 96.84% examples, 756139 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:22:55: EPOCH 5 - PROGRESS: at 98.40% examples, 754526 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 21:22:56: EPOCH 5 - PROGRESS: at 99.76% examples, 752912 words/s, in_qsize 8, out_qsize 0\n",
      "INFO - 21:22:56: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:22:56: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:22:56: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:22:56: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:22:56: EPOCH - 5 : training on 63696185 raw words (45787334 effective words) took 60.8s, 752723 effective words/s\n",
      "INFO - 21:22:56: training on a 318480925 raw words (228928716 effective words) took 301.8s, 758547 effective words/s\n",
      "INFO - 21:22:56: precomputing L2-norms of word weight vectors\n",
      "INFO - 21:22:56: saving Word2Vec object under 300features_40minwords_10context.wv.model, separately None\n",
      "INFO - 21:22:56: not storing attribute vectors_norm\n",
      "INFO - 21:22:56: not storing attribute cum_table\n",
      "INFO - 21:22:57: saved 300features_40minwords_10context.wv.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x2a9bf73ef0>"
      ]
     },
     "execution_count": 1510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec(sentences) # uncomment if we want to build the model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1511,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1136,
     "status": "ok",
     "timestamp": 1571209328730,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "EUd3499KEori",
    "outputId": "236b221b-fa92-41ce-81b3-6b8d1653656d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:26:29: loading Word2Vec object from 300features_40minwords_10context.wv.model\n",
      "INFO - 21:26:30: loading wv recursively from 300features_40minwords_10context.wv.model.wv.* with mmap=None\n",
      "INFO - 21:26:30: setting ignored attribute vectors_norm to None\n",
      "INFO - 21:26:30: loading vocabulary recursively from 300features_40minwords_10context.wv.model.vocabulary.* with mmap=None\n",
      "INFO - 21:26:30: loading trainables recursively from 300features_40minwords_10context.wv.model.trainables.* with mmap=None\n",
      "INFO - 21:26:30: setting ignored attribute cum_table to None\n",
      "INFO - 21:26:30: loaded 300features_40minwords_10context.wv.model\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec.load(\"300features_40minwords_10context.wv.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hf2rmxVBeJeN"
   },
   "source": [
    "### Review most similar words\n",
    "Get initial evaluation of the word vectors by analyzing the most similar words for a few interesting words in the text. \n",
    "\n",
    "Choose words yourself, and find the most similar words to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1512,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1119,
     "status": "ok",
     "timestamp": 1571209328731,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "VinwjWHsFeYv",
    "outputId": "d1972700-b6f8-4b91-d4ba-8069840fa04d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:26:33: precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('hound', 0.6072163581848145),\n",
       " ('puppy', 0.5779114961624146),\n",
       " ('cat', 0.5628559589385986),\n",
       " ('barkin', 0.5582836270332336),\n",
       " ('barking', 0.5339473485946655),\n",
       " ('frog', 0.5248117446899414),\n",
       " ('doggy', 0.5087523460388184),\n",
       " ('bark', 0.5061835050582886),\n",
       " ('hog', 0.5035767555236816),\n",
       " ('dogs', 0.5019047260284424)]"
      ]
     },
     "execution_count": 1512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"dog\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1513,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1112,
     "status": "ok",
     "timestamp": 1571209328732,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "sbGzgOYQFoKf",
    "outputId": "b29d7ca5-dad7-49dc-d31d-401e9e9458da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('baby', 0.555811882019043),\n",
       " ('loving', 0.5440404415130615),\n",
       " ('heart', 0.522280216217041),\n",
       " ('oh', 0.4947924017906189),\n",
       " ('true', 0.48665851354599),\n",
       " ('you', 0.48389655351638794),\n",
       " ('unconditional', 0.48382511734962463),\n",
       " ('darling', 0.47346723079681396),\n",
       " ('lovin', 0.4705849885940552),\n",
       " ('darlin', 0.4701448380947113)]"
      ]
     },
     "execution_count": 1513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"love\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1514,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1106,
     "status": "ok",
     "timestamp": 1571209328733,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "gzzC597cFzVp",
    "outputId": "ad824bf5-fb0b-4dea-a347-e098f511ef44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('battle', 0.6661872863769531),\n",
       " ('wars', 0.6377437114715576),\n",
       " ('waged', 0.629664421081543),\n",
       " ('waging', 0.6170507669448853),\n",
       " ('civil', 0.5605376958847046),\n",
       " ('battlefield', 0.536693811416626),\n",
       " ('warfare', 0.5238355398178101),\n",
       " ('fighting', 0.5131027698516846),\n",
       " ('tug', 0.5119432210922241),\n",
       " ('battles', 0.5112775564193726)]"
      ]
     },
     "execution_count": 1514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"war\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1515,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1100,
     "status": "ok",
     "timestamp": 1571209328733,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "DyDYOaXYF2V6",
    "outputId": "9d1437da-4153-4898-d9ee-b27a09c557c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bluest', 0.5243340730667114),\n",
       " ('gray', 0.5117449760437012),\n",
       " ('grey', 0.4957510530948639),\n",
       " ('hue', 0.44540661573410034),\n",
       " ('cloudless', 0.4412332773208618),\n",
       " ('misty', 0.4387950301170349),\n",
       " ('starry', 0.4387114644050598),\n",
       " ('cloudy', 0.4215405583381653),\n",
       " ('green', 0.41407644748687744),\n",
       " ('red', 0.41392406821250916)]"
      ]
     },
     "execution_count": 1515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"blue\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1516,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1300,
     "status": "ok",
     "timestamp": 1571209328945,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "OzSRHgcJF4u5",
    "outputId": "5b51c344-1c16-4b30-fe21-d96eeb51e214"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('france', 0.6714458465576172),\n",
       " ('hilton', 0.6198512315750122),\n",
       " ('spain', 0.6048012375831604),\n",
       " ('italy', 0.5840489268302917),\n",
       " ('tokyo', 0.5763559937477112),\n",
       " ('london', 0.5726921558380127),\n",
       " ('venice', 0.5575970411300659),\n",
       " ('rome', 0.5369668006896973),\n",
       " ('miami', 0.5305115580558777),\n",
       " ('milan', 0.5288726687431335)]"
      ]
     },
     "execution_count": 1516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"paris\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XidhoNY_eJeR"
   },
   "source": [
    "### Word Vectors Algebra\n",
    "We've seen in class examples of algebraic games on the word vectors (e.g. man - woman + king = queen ). \n",
    "\n",
    "Try a few vector algebra terms, and evaluate how well they work. Try to use the Cosine distance and compare it to the Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1517,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 502,
     "status": "ok",
     "timestamp": 1571209342454,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "4HDL8dOePc6q",
    "outputId": "c3e52cc5-23cd-411c-be65-d74e15d2f7a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.6248096227645874),\n",
       " ('kings', 0.42998164892196655),\n",
       " ('crowning', 0.4081261157989502),\n",
       " ('crown', 0.399611234664917),\n",
       " ('homecoming', 0.39911770820617676),\n",
       " ('goddess', 0.3796178102493286),\n",
       " ('crowned', 0.3766542077064514),\n",
       " ('reigning', 0.3633894622325897),\n",
       " ('throne', 0.36280179023742676),\n",
       " ('princess', 0.36060717701911926)]"
      ]
     },
     "execution_count": 1517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1518,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 691,
     "status": "ok",
     "timestamp": 1571209343193,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "aU9F91ciPPCN",
    "outputId": "54254295-50be-4644-9b80-fb7c3854e9e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.9129402041435242),\n",
       " ('homecoming', 0.7548413276672363),\n",
       " ('crowning', 0.749069094657898),\n",
       " ('crown', 0.7434477806091309),\n",
       " ('kings', 0.7417984008789062),\n",
       " ('goddess', 0.7413286566734314),\n",
       " ('princess', 0.7282884120941162),\n",
       " ('beauty', 0.7236695885658264),\n",
       " ('latifah', 0.7194628715515137),\n",
       " ('newborn', 0.719289243221283)]"
      ]
     },
     "execution_count": 1518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1520,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 669,
     "status": "ok",
     "timestamp": 1571209343816,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "uLJqiDuFmgQV",
    "outputId": "4080337c-8106-43cc-ee31-ac2b49ec5dd8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 0.7157250046730042),\n",
       " ('queen', 0.6248096227645874),\n",
       " ('kings', 0.4299815893173218),\n",
       " ('crowning', 0.4081261157989502),\n",
       " ('crown', 0.3996112644672394),\n",
       " ('homecoming', 0.39911770820617676),\n",
       " ('goddess', 0.3796178102493286),\n",
       " ('crowned', 0.3766542077064514),\n",
       " ('woman', 0.3646465539932251),\n",
       " ('reigning', 0.3633894622325897)]"
      ]
     },
     "execution_count": 1520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = w2v_model.wv['king'] - w2v_model.wv['man'] + w2v_model.wv['woman']\n",
    "w2v_model.wv.similar_by_vector(vector, topn=10, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1521,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 382,
     "status": "ok",
     "timestamp": 1571209344111,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "f1lZ-n_xeJeR",
    "outputId": "a1258eea-9532-47c8-eab0-67f5233f3d9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distance =  0.54114074\n",
      "Euclidean distance =  1.3017702\n"
     ]
    }
   ],
   "source": [
    "# Cosine similarity\n",
    "cosine_distance = w2v_model.wv.similarity(\"woman\", \"girl\")\n",
    "print(\"Cosine distance = \", cosine_distance)\n",
    "\n",
    "import numpy as np\n",
    "euclidean_distance = np.linalg.norm(w2v_model.wv[\"women\"] - w2v_model.wv[\"girl\"])\n",
    "print(\"Euclidean distance = \", euclidean_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HmvadkFfeJeU"
   },
   "source": [
    "## Sentiment Analysis\n",
    "Estimate sentiment of words using word vectors.  \n",
    "In this section, we'll use the SemEval-2015 English Twitter Sentiment Lexicon.  \n",
    "The lexicon was used as an official test set in the SemEval-2015 shared Task #10: Subtask E, and contains a polarity score for words in range -1 (negative) to 1 (positive) - http://saifmohammad.com/WebPages/SCL.html#OPP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gryJj-R_eJeU"
   },
   "source": [
    "Build a classifier for the sentiment of a word given its word vector. Split the data to a train and test sets, and report the model performance on both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XphYXEzfeJeV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rN905F1GeJeY"
   },
   "source": [
    "Use your trained model from the previous question to predict the sentiment score of words in the lyrics corpus that are not part of the original sentiment dataset. Review the words with the highest positive and negative sentiment. Do the results make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WjzZmCAReJeY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YgtlmAEbeJeb"
   },
   "source": [
    "### Visualize Word Vectors\n",
    "In this section, you'll plot words on a 2D grid based on their inner similarity. We'll use the tSNE transformation to reduce dimensions from 300 to 2. You can get sample code from https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial or other tutorials online.\n",
    "\n",
    "Perform the following:\n",
    "- Keep only the 3,000 most frequent words (after removing stopwords)\n",
    "- For this list, compute for each word its relative abundance in each of the genres\n",
    "- Compute the ratio between the proportion of each word in each genre and the proportion of the word in the entire corpus (the background distribution)\n",
    "- Pick the top 50 words for each genre. These words give good indication for that genre. Join the words from all genres into a single list of top significant words. \n",
    "- Compute tSNE transformation to 2D for all words, based on their word vectors\n",
    "- Plot the list of the top significant words in 2D. Next to each word output its text. The color of each point should indicate the genre for which it is most significant.\n",
    "\n",
    "You might prefer to use a different number of points or a slightly different methodology for improved results.  \n",
    "Analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1522,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jz_Meze2eJeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300features_40minwords_10context.wv.model\r\n",
      "380000-lyrics-from-metrolyrics.zip\r\n",
      "DL_rnn_text_classification_generation.ipynb\r\n",
      "DL_word_embedding_assignment.ipynb\r\n",
      "README.md\r\n",
      "best-model.model\r\n",
      "best-model.pt\r\n",
      "cnn.py\r\n",
      "environment.yml\r\n",
      "test.csv\r\n",
      "train.csv\r\n",
      "val.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SqsWYOmceJed"
   },
   "source": [
    "## Text Classification\n",
    "In this section, you'll build a text classifier, determining the genre of a song based on its lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>song</th>\n",
       "      <th>year</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ego-remix</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Oh baby, how you doing?\\nYou know I'm gonna cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>then-tell-me</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>playin' everything so easy,\\nit's like you see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>honesty</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>If you search\\nFor tenderness\\nIt isn't hard t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>you-are-my-rock</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Oh oh oh I, oh oh oh I\\n[Verse 1:]\\nIf I wrote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>black-culture</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Party the people, the people the party it's po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362232</td>\n",
       "      <td>362232</td>\n",
       "      <td>who-am-i-drinking-tonight</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>I gotta say\\nBoy, after only just a couple of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362233</td>\n",
       "      <td>362233</td>\n",
       "      <td>liar</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>I helped you find her diamond ring\\nYou made m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362234</td>\n",
       "      <td>362234</td>\n",
       "      <td>last-supper</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>Look at the couple in the corner booth\\nLooks ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362235</td>\n",
       "      <td>362235</td>\n",
       "      <td>christ-alone-live-in-studio</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>When I fly off this mortal earth\\nAnd I'm meas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362236</td>\n",
       "      <td>362236</td>\n",
       "      <td>amen</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>I heard from a friend of a friend of a friend ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>242580 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index                         song  year           artist    genre  \\\n",
       "0            0                    ego-remix  2009  beyonce-knowles      Pop   \n",
       "1            1                 then-tell-me  2009  beyonce-knowles      Pop   \n",
       "2            2                      honesty  2009  beyonce-knowles      Pop   \n",
       "3            3              you-are-my-rock  2009  beyonce-knowles      Pop   \n",
       "4            4                black-culture  2009  beyonce-knowles      Pop   \n",
       "...        ...                          ...   ...              ...      ...   \n",
       "362232  362232    who-am-i-drinking-tonight  2012       edens-edge  Country   \n",
       "362233  362233                         liar  2012       edens-edge  Country   \n",
       "362234  362234                  last-supper  2012       edens-edge  Country   \n",
       "362235  362235  christ-alone-live-in-studio  2012       edens-edge  Country   \n",
       "362236  362236                         amen  2012       edens-edge  Country   \n",
       "\n",
       "                                                   lyrics  \n",
       "0       Oh baby, how you doing?\\nYou know I'm gonna cu...  \n",
       "1       playin' everything so easy,\\nit's like you see...  \n",
       "2       If you search\\nFor tenderness\\nIt isn't hard t...  \n",
       "3       Oh oh oh I, oh oh oh I\\n[Verse 1:]\\nIf I wrote...  \n",
       "4       Party the people, the people the party it's po...  \n",
       "...                                                   ...  \n",
       "362232  I gotta say\\nBoy, after only just a couple of ...  \n",
       "362233  I helped you find her diamond ring\\nYou made m...  \n",
       "362234  Look at the couple in the corner booth\\nLooks ...  \n",
       "362235  When I fly off this mortal earth\\nAnd I'm meas...  \n",
       "362236  I heard from a friend of a friend of a friend ...  \n",
       "\n",
       "[242580 rows x 6 columns]"
      ]
     },
     "execution_count": 1525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's first filter all songs with no genre (null or have \"Not Available\" genre)\n",
    "data = data[data[\"genre\"] != \"Not Available\"]\n",
    "data = data[data[\"genre\"].notnull()]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x28c2819be0>"
      ]
     },
     "execution_count": 1528,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['genre'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1529,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 496,
     "status": "ok",
     "timestamp": 1571209350239,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "taGaRdEyAVzt",
    "outputId": "f10c0a98-955c-4d76-ba26-f9265fbe99fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 8, 8, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 1529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# creating labelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# Converting string labels into numbers.\n",
    "genres_encoded = le.fit_transform(data[\"genre\"].tolist())\n",
    "genres_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1530,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 600,
     "status": "ok",
     "timestamp": 1571209350888,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "e-xFPz1UZSrM",
    "outputId": "9ae6a3a6-d473-41db-8ed1-62138d079d0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country 0\n",
      "Electronic 1\n",
      "Folk 2\n",
      "Hip-Hop 3\n",
      "Indie 4\n",
      "Jazz 5\n",
      "Metal 6\n",
      "Other 7\n",
      "Pop 8\n",
      "R&B 9\n",
      "Rock 10\n"
     ]
    }
   ],
   "source": [
    "classes = list(le.classes_)\n",
    "codes = le.transform(list(le.classes_))\n",
    "for clss, code in zip(classes, codes):\n",
    "  print(clss, code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5DmqzoiveJee"
   },
   "source": [
    "### Text classification using Bag-of-Words\n",
    "Build a Naive Bayes classifier based on the bag of Words.  \n",
    "You will need to divide your dataset into a train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1531,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nu2XZ_5B6Ycv"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split songs dataset to train, test and validation data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\"lyrics\"].values,\n",
    "                                                    genres_encoded, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,\n",
    "                                                  y_train,\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1532,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 517,
     "status": "ok",
     "timestamp": 1571209353756,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "tT0zmYA8E1Mq",
    "outputId": "c1d0270f-c9f3-4a87-d6f1-8b73561edc55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155251,)"
      ]
     },
     "execution_count": 1532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1533,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RKpy9kfZ5-y1"
   },
   "outputs": [],
   "source": [
    "def clean_X(X):\n",
    "    clean_X = []\n",
    "\n",
    "    # Loop over each document; create an index i that goes from 0 to the length\n",
    "    # of the train list \n",
    "    for i, val in enumerate(X):\n",
    "        # Call our function for each one, and add the result to the list of clean reviews\n",
    "        if ((i + 1) % 50000 == 0):\n",
    "            print(\"Lyrics %d of %d\\n\" % (i + 1, X.size))\n",
    "        clean_X.append(\" \".join(document_to_words(val)))\n",
    "  \n",
    "    return clean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1534,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11421,
     "status": "ok",
     "timestamp": 1571209366062,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "SCwkfz0Qa7Uc",
    "outputId": "c1d20404-0c32-473c-94fe-3cba4363f50d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyrics 50000 of 155251\n",
      "\n",
      "Lyrics 100000 of 155251\n",
      "\n",
      "Lyrics 150000 of 155251\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_X_train = clean_X(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1535,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10953,
     "status": "ok",
     "timestamp": 1571209366063,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "RUoMLLvH-38T",
    "outputId": "1bbbb13f-20b3-402d-f95e-43c14c2a16ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['if i didn t already love you i d fall in love tonight',\n",
       " 'i m a game that you used to play and i m a plan that you didn t lay so well and i m a fire that burns in your mind so close your eyes i m a memory i m a love that you bought for a song and i m a voice on a green telephone and i m a day that lasted so long so close your eyes i m a memory i m a dream that comes with the night and i m a face that fades with the light and i m a tear that falls out of sight so close your eyes i m a memory i m a love that you bought for a song and i m a voice on a green telephone and i m a day that lasted so long so close your eyes i m a memory',\n",
       " 'you say your hurting is over it feels like you re back from the dead but still i can t believe it s over and i can t get your sight your scent from my head it seems like these moments are lasting forever the times when we felt alive you carried my heart in the midst of this battle in your hands this time i won t try to reach you you re already too far gone slipped past and i didn t notice did we ever sing the same song two lives two books of illusion a chapter in the essence of time the story we wrote in one season came to an end in one night remember the times when we started believing that everything would be alright you carried my heart in the midst of this battle in your hands and i ll wait for you i ll become something new and i ll sing for you until this dream comes true']"
      ]
     },
     "execution_count": 1535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_X_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1536,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20101,
     "status": "ok",
     "timestamp": 1571209376916,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "H6IFohnbbXZy",
    "outputId": "7040797e-3e68-40aa-e7cf-7afea68d12ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyrics 50000 of 155251\n",
      "\n",
      "Lyrics 100000 of 155251\n",
      "\n",
      "Lyrics 150000 of 155251\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_X_test = clean_X(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1537,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cBNLzHnEeJef"
   },
   "outputs": [],
   "source": [
    "def vectorize_features(X, vectorizer):\n",
    "      # fit_transform() does two functions: First, it fits the model\n",
    "      # and learns the vocabulary; second, it transforms our training data\n",
    "      # into feature vectors. The input to fit_transform should be a list of \n",
    "      # strings.\n",
    "    features = vectorizer.fit_transform(X)\n",
    "\n",
    "      # Numpy arrays are easy to work with, so convert the result to an \n",
    "      # array\n",
    "    features = features.toarray()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1538,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yww-Gfckbdy3"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None, \n",
    "                             preprocessor = None,\n",
    "                             stop_words = None,\n",
    "                             max_features = 5000) \n",
    "\n",
    "X_train_features = vectorize_features(X_train, vectorizer)\n",
    "X_test_features = vectorize_features(X_test, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1539,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 57708,
     "status": "ok",
     "timestamp": 1571209419631,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "IyTDbm5SUqXq",
    "outputId": "30204c91-5b0e-4ef1-d943-39f329069309"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 1 0 1 2]\n",
      " [1 0 1 0 1 1 0 1]]\n",
      "['ate', 'banana', 'bananas', 'eat', 'in', 'spring', 'summer', 'the']\n"
     ]
    }
   ],
   "source": [
    "# See how the Vectorizer works\n",
    "features = vectorizer.fit_transform(['i eat the banana in the summer', 'in the spring i ate bananas'])\n",
    "print(features.toarray())\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1540,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 80018,
     "status": "ok",
     "timestamp": 1571209442910,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "S2_vHi-lD3mU",
    "outputId": "a531fcae-4fd0-41b9-defb-d01053e47fd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 1540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "nbc = MultinomialNB()\n",
    "\n",
    "#Train the model using the training sets\n",
    "nbc.fit(X_train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1541,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0gFRPl5kc2qE"
   },
   "outputs": [],
   "source": [
    "#Predict the response for test dataset\n",
    "y_pred = nbc.predict(X_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GAAX5udXeJeh"
   },
   "source": [
    "Show the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1542,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 97608,
     "status": "ok",
     "timestamp": 1571209464945,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "QZMp1imVcTxQ",
    "outputId": "7a9144e7-8f37-438f-ee6d-aaad6688e7c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48516, 5000)\n",
      "(48516,)\n",
      "48516\n"
     ]
    }
   ],
   "source": [
    "print(X_test_features.shape)\n",
    "print(y_test.shape)\n",
    "print(len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1543,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 93349,
     "status": "ok",
     "timestamp": 1571209464945,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "VCZgDVrueJei",
    "outputId": "7553c862-0a61-4d96-9174-92dc834751dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0    28   780   478     4     8     6  1524     1     0    25]\n",
      " [    2    22   306   325     4    13    25   771     5     1   100]\n",
      " [    0    11   109    72     0     2     6   168     3     0    34]\n",
      " [    1    21  1240  1230     0     8    24  2409     5     0    37]\n",
      " [    1     9   133   105     0     4     2   353     1     0     8]\n",
      " [    0    17   343   325     3     5     8   868     1     0    44]\n",
      " [    2    64  1269  1024     5    18    49  2127    14     0   249]\n",
      " [    1    18   212   255     3     6     6   592     5     0     9]\n",
      " [    3   130  1458  1552    18    30    64  4715    20     0    99]\n",
      " [    0    10   101   115     0     2     4   447     0     0     4]\n",
      " [    4   270  4836  3980    52    95   167 11746    45     0   583]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fn3Gbnu1eJek"
   },
   "source": [
    "Show the classification report - precision, recall, f1 for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1544,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 92120,
     "status": "ok",
     "timestamp": 1571209464946,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "E0wmjGdveJel",
    "outputId": "19bb575f-eb7d-49d6-8e55-e9b571aa3624"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Country       0.00      0.00      0.00      2854\n",
      "  Electronic       0.04      0.01      0.02      1574\n",
      "        Folk       0.01      0.27      0.02       405\n",
      "     Hip-Hop       0.13      0.25      0.17      4975\n",
      "       Indie       0.00      0.00      0.00       616\n",
      "        Jazz       0.03      0.00      0.01      1614\n",
      "       Metal       0.14      0.01      0.02      4821\n",
      "       Other       0.02      0.53      0.04      1107\n",
      "         Pop       0.20      0.00      0.00      8089\n",
      "         R&B       0.00      0.00      0.00       683\n",
      "        Rock       0.49      0.03      0.05     21778\n",
      "\n",
      "    accuracy                           0.05     48516\n",
      "   macro avg       0.10      0.10      0.03     48516\n",
      "weighted avg       0.28      0.05      0.04     48516\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "opFVcx_ueJen"
   },
   "source": [
    "### Text classification using Word Vectors\n",
    "#### Average word vectors\n",
    "Do the same, using a classifier that averages the word vectors of words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1545,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W5w9jxIljIgR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def avg_word_vector(X):\n",
    "    avg_vectors = [];\n",
    "    # todo: can be better implemented with numpy\n",
    "    for i, val in enumerate(X):\n",
    "        total_vector = np.zeros(w2v_model.vector_size)\n",
    "        words = val.split(\" \")\n",
    "        for word in words:\n",
    "            total_vector += w2v_model[word] if word in w2v_model.wv.vocab else np.zeros(w2v_model.vector_size) \n",
    "        avg_vector = total_vector / len(words)\n",
    "        avg_vectors.append(avg_vector)\n",
    "    return avg_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1546,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 227628,
     "status": "ok",
     "timestamp": 1571209756520,
     "user": {
      "displayName": "Asaf AM",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAI3sw08QX80dL9h_fnKeiw8b8bhkNvz8sRYPSfZw=s64",
      "userId": "00322487638289632571"
     },
     "user_tz": -180
    },
    "id": "ZpSfTQ0djFEd",
    "outputId": "9558285d-4cf9-4c25-e0df-d2577fe023c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/biu-python/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "X_train_avg_vc = avg_word_vector(X_train)\n",
    "X_test_avg_vc = avg_word_vector(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1547,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "scaler = StandardScaler()  \n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(X_train_avg_vc)  \n",
    "X_train_avg_vc_scaled = scaler.transform(X_train_avg_vc)  \n",
    "# apply same transformation to test data\n",
    "X_test_avg_vc_scaled = scaler.transform(X_test_avg_vc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1548,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2zrlkBybeJeo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/biu-python/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    " \n",
    "clf = MLPClassifier(hidden_layer_sizes=(50,))\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(X_train_avg_vc_scaled, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test_avg_vc_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1549,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lO38v6yRi_yr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  858     4     2    16     0    61    15     2   339     0  1557]\n",
      " [   14    54     0    82     0     8    72     2   315     0  1027]\n",
      " [   22     1    33     6     0     2    16     1    47     0   277]\n",
      " [   17    24     0  3415     0     4    41    11   504     1   958]\n",
      " [   18     1     0     9     2     3    10     1    79     0   493]\n",
      " [  114     5     0    28     0   214    16    19   329     0   889]\n",
      " [   12    16     0    81     0     4  2273     1   148     2  2284]\n",
      " [   33     7     3   101     0    70    33    29   269     1   561]\n",
      " [  183    30     5   290     0    86   134    21  2909     0  4431]\n",
      " [   12     3     0    28     0    12    10     0   186     1   431]\n",
      " [  620    67     9   322     0   149   826    26  2528     5 17226]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1550,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HyMzQjx9jCeC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Country       0.45      0.30      0.36      2854\n",
      "  Electronic       0.25      0.03      0.06      1574\n",
      "        Folk       0.63      0.08      0.14       405\n",
      "     Hip-Hop       0.78      0.69      0.73      4975\n",
      "       Indie       1.00      0.00      0.01       616\n",
      "        Jazz       0.35      0.13      0.19      1614\n",
      "       Metal       0.66      0.47      0.55      4821\n",
      "       Other       0.26      0.03      0.05      1107\n",
      "         Pop       0.38      0.36      0.37      8089\n",
      "         R&B       0.10      0.00      0.00       683\n",
      "        Rock       0.57      0.79      0.66     21778\n",
      "\n",
      "    accuracy                           0.56     48516\n",
      "   macro avg       0.49      0.26      0.28     48516\n",
      "weighted avg       0.54      0.56      0.52     48516\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_8Wu_h1zeJeq"
   },
   "source": [
    "#### TfIdf Weighting\n",
    "Do the same, using a classifier that averages the word vectors of words in the document, weighting each word by its TfIdf.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyrics 50000 of 242580\n",
      "\n",
      "Lyrics 100000 of 242580\n",
      "\n",
      "Lyrics 150000 of 242580\n",
      "\n",
      "Lyrics 200000 of 242580\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['oh baby how you doing you know i m gonna cut right to the chase some women were made but me myself i like to think that i was created for a special purpose you know what s more special than you you feel me it s on baby let s get lost you don t need to call into work cause you re the boss for real want you to show me how you feel i consider myself lucky that s a big deal why well you got the key to my heart but you ain t gonna need it i d rather you open up my body and show me secrets you didn t know was inside no need for me to lie it s too big it s too wide it s too strong it won t fit it s too much it s too tough he talk like this cause he can back it up he got a big ego such a huge ego i love his big ego it s too much he walk like this cause he can back it up usually i m humble right now i don t choose you can leave with me or you could have the blues some call it arrogant i call it confident you decide when you find on what i m working with damn i know i m killing you with them legs better yet them thighs matter a fact it s my smile or maybe my eyes boy you a site to see kind of something like me it s too big it s too wide it s too strong it won t fit it s too much it s too tough i talk like this cause i can back it up i got a big ego such a huge ego but he love my big ego it s too much i walk like this cause i can back it up i i walk like this cause i can back it up i i talk like this cause i can back it up i i can back it up i can back it up i walk like this cause i can back it up it s too big it s too wide it s too strong it won t fit it s too much it s too tough he talk like this cause he can back it up he got a big ego such a huge ego such a huge ego i love his big ego it s too much he walk like this cause he can back it up ego so big you must admit i got every reason to feel like i m that bitch ego so strong if you ain t know i don t need no beat i can sing it with piano',\n",
       " 'playin everything so easy it s like you seem so sure still your ways you dont see i m not sure if they re for me then things come right along our way though we didn t truly ask it seems as if they re gonna linger with every delight they bring just like what you have truly seemed i m trying to think of what you really want to say even through my darkest day you might want to leave me feeling strange about you like you re gonna let me know when words then slipped out of you when words dont come so easy to say you just leave me feeling come what may though i want things coming from your way i say to you you bore me all the time when you seem to hold back all in you all that you want to let me know why dont you have the courage speak up and i ll listen if you truly want me to know then tell me is there something wrong with you and you seem fastened there it sounds as if there ll be a melody if things in you are let out and then i will feel alright when you sleep do you feel the same exactly as i do i really want to hear things from you though i ve felt something new eversince you acted that way if i go would you still mind telling me if i stay you seem to let the days go by if you truly want to let me know then tell me',\n",
       " 'if you search for tenderness it isn t hard to find you can have the love you need to live but if you look for truthfulness you might just as well be blind it always seems to be so hard to give chorus honesty is such a lonely word everyone is so untrue honesty is hardly ever heard and mostly what i need from you i can always find someone to say they sympathize if i wear my heart out on my sleeve but i don t want some pretty face to tell me pretty lies all i want is someone to believe chorus i can find a lover i can find a friend i can have security until the bitter end anyone can comfort me with promises again i know i know when i m deep inside of me don t be too concerned i won t ask for nothin while i m gone but when i want sincerity tell me where else can i turn when you re the one that i depend upon chorus']"
      ]
     },
     "execution_count": 1551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = clean_X(data[\"lyrics\"].values)\n",
    "X[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1552,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zdse6oa9eJer"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=5)\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_tfidf[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1554,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split songs dataset to train, test and validation data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf,\n",
    "                                                    genres_encoded, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,\n",
    "                                                  y_train,\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1555,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 1555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:100].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1556,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/biu-python/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    " \n",
    "clf = MLPClassifier(hidden_layer_sizes=(5,))\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1557,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lunpCWm-il0C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1073    20    26    59    27   141    67    34   341    51  1015]\n",
      " [   43   221     8   112    20    27   117    38   282    15   691]\n",
      " [   28     4    48    12    15    14    23    16    61     4   180]\n",
      " [   63    92    13  3375    10    57   106    71   454    59   675]\n",
      " [   32    11     4    24    47     9    32    18    95     8   336]\n",
      " [  159    10    13    75     9   414    24    80   266    31   533]\n",
      " [  100   119    22   217    76    31  2211    42   313    31  1659]\n",
      " [   41    34    10   107    21   134    45   106   234     7   368]\n",
      " [  364   210    44   451    97   229   316   147  3174    71  2986]\n",
      " [   42     8     4    62     2    35    28     8   138    82   274]\n",
      " [ 1257   446   175   830   335   500  1602   342  2868   212 13211]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1558,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tk5_GjgximgO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Country       0.34      0.38      0.35      2854\n",
      "  Electronic       0.19      0.14      0.16      1574\n",
      "        Folk       0.13      0.12      0.12       405\n",
      "     Hip-Hop       0.63      0.68      0.66      4975\n",
      "       Indie       0.07      0.08      0.07       616\n",
      "        Jazz       0.26      0.26      0.26      1614\n",
      "       Metal       0.48      0.46      0.47      4821\n",
      "       Other       0.12      0.10      0.11      1107\n",
      "         Pop       0.39      0.39      0.39      8089\n",
      "         R&B       0.14      0.12      0.13       683\n",
      "        Rock       0.60      0.61      0.60     21778\n",
      "\n",
      "    accuracy                           0.49     48516\n",
      "   macro avg       0.30      0.30      0.30     48516\n",
      "weighted avg       0.49      0.49      0.49     48516\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RwndE40EeJet"
   },
   "source": [
    "### Text classification using ConvNet\n",
    "Do the same, using a ConvNet.  \n",
    "The ConvNet should get as input a 2D matrix where each column is an embedding vector of a single word, and words are in order. Use zero padding so that all matrices have a similar length.  \n",
    "Some songs might be very long. Trim them so you keep a maximum of 128 words (after cleaning stop words and rare words).  \n",
    "Initialize the embedding layer using the word vectors that you've trained before, but allow them to change during training.  \n",
    "\n",
    "Extra: Try training the ConvNet with 2 slight modifications:\n",
    "1. freezing the the weights trained using Word2vec (preventing it from updating)\n",
    "1. random initialization of the embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BL5-LsxNeJeu"
   },
   "source": [
    "You are encouraged to try this question on your own.  \n",
    "\n",
    "You might prefer to get ideas from the paper \"Convolutional Neural Networks for Sentence Classification\" (Kim 2014, [link](https://arxiv.org/abs/1408.5882)).\n",
    "\n",
    "There are several implementations of the paper code in PyTorch online (see for example [this repo](https://github.com/prakashpandey9/Text-Classification-Pytorch) for a PyTorch implementation of CNN and other architectures for text classification). If you get stuck, they might provide you with a reference for your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300features_40minwords_10context\r\n",
      "380000-lyrics-from-metrolyrics.zip\r\n",
      "DL_rnn_text_classification_generation.ipynb\r\n",
      "DL_word_embedding_assignment.ipynb\r\n",
      "README.md\r\n",
      "best-model.pt\r\n",
      "cnn.py\r\n",
      "environment.yml\r\n",
      "temp.md\r\n",
      "temp.md.pt\r\n",
      "test.csv\r\n",
      "train.csv\r\n",
      "val.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1382,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('380000-lyrics-from-metrolyrics.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all data that is not null\n",
    "data = raw_data[raw_data['lyrics'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 444kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy in /anaconda3/envs/biu-python/lib/python3.6/site-packages (from torchtext) (1.17.2)\n",
      "Requirement already satisfied: requests in /anaconda3/envs/biu-python/lib/python3.6/site-packages (from torchtext) (2.22.0)\n",
      "Requirement already satisfied: torch in /anaconda3/envs/biu-python/lib/python3.6/site-packages (from torchtext) (1.3.0)\n",
      "Requirement already satisfied: six in /anaconda3/envs/biu-python/lib/python3.6/site-packages (from torchtext) (1.12.0)\n",
      "Requirement already satisfied: tqdm in /anaconda3/envs/biu-python/lib/python3.6/site-packages (from torchtext) (4.36.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/envs/biu-python/lib/python3.6/site-packages (from requests->torchtext) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/envs/biu-python/lib/python3.6/site-packages (from requests->torchtext) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /anaconda3/envs/biu-python/lib/python3.6/site-packages (from requests->torchtext) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /anaconda3/envs/biu-python/lib/python3.6/site-packages (from requests->torchtext) (1.24.3)\n",
      "Installing collected packages: torchtext\n",
      "Successfully installed torchtext-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split songs dataset to train, test and validation data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\"lyrics\"].values,\n",
    "                                                    data[\"genre\"].values, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,\n",
    "                                                  y_train,\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It must be summer\\n'Cause the days are long\\nAnd I dial your number\\nBut you're gone, gone, gone\\nI'd set out searching\\nBut the car won't start\\nAnd it must be summer\\n'Cause I'm falling apart\\nI try your sister\\nOn the Jersey Shore\\nShe said you might be stopping by\\nBut she's not sure\\nSo I call your mother\\nOn Long Island Sound\\nShe said it must be summer\\n'Cause you're never around\\nAnd the sun is beating me senseless\\nI feel defenseless like a dying lamb\\nI don't wan't to lie by the oceanside\\nDon't want to play in the sand\\nCan't you understand?\\nCan't you understand?\\nIt must be summer\\n'Cause the streets are bare\\nAnd I try your number\\nBut you're just not there\\nAnd the sun keeps shining\\n'Til it's dead and gone\\nAnd it must be summer\\n'Cause I can't go on\""
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.DataFrame({'lyrics': X_train, 'genre': y_train})\n",
    "test_df = pd.DataFrame({'lyrics': X_test, 'genre': y_test})\n",
    "val_df = pd.DataFrame({'lyrics': X_val, 'genre': y_val})\n",
    "\n",
    "train_df.to_csv(r'train.csv')\n",
    "test_df.to_csv(r'test.csv')\n",
    "val_df.to_csv(r'val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300features_40minwords_10context\r\n",
      "380000-lyrics-from-metrolyrics.zip\r\n",
      "DL_rnn_text_classification_generation.ipynb\r\n",
      "DL_word_embedding_assignment.ipynb\r\n",
      "README.md\r\n",
      "cnn.py\r\n",
      "environment.yml\r\n",
      "test.csv\r\n",
      "train.csv\r\n",
      "val.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   lyrics    genre\n",
      "0       It must be summer\\n'Cause the days are long\\nA...     Rock\n",
      "1       Mug mug mug\\nCoffee mug\\nGonna clear the haze\\...     Rock\n",
      "2       We stepped together in the river\\nWe traded fe...     Rock\n",
      "3       Alright Im gonna show ya how different I am\\nE...     Rock\n",
      "4       - this crime - for him\\n- desire - noone\\n- se...     Rock\n",
      "...                                                   ...      ...\n",
      "170558  I couldn't think of someone that Id rather see...     Rock\n",
      "170559  Virgil Caine is the name, and I served on the ...     Rock\n",
      "170560  The reason why I've called us all together\\nIs...  Country\n",
      "170561  California's burning, burning\\nBurning to the ...     Rock\n",
      "170562  I've got a secret.\\nIt's on the tip of my tong...    Metal\n",
      "\n",
      "[170563 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add <pad> special word to the dictionary\n",
    "PAD = \"<pad>\"\n",
    "w2v_model.wv[\"<pad>\"] = np.zeros(300,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def process_document(text, remove_panctuations=True, \n",
    "                        remove_stopwords=True, remove_unknown_words=True, padding=300):\n",
    "    \n",
    "    if remove_panctuations:\n",
    "      text = re.sub(\"[^a-zA-Z0-9]\",\" \", text)\n",
    "    #\n",
    "    # Convert words to lower case and split them\n",
    "    words = text.lower().split()\n",
    "    #\n",
    "    # Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w.lower() in stops]  \n",
    "        \n",
    "    if remove_unknown_words:\n",
    "        words = [w for w in words if w in w2v_model.wv]  \n",
    "    \n",
    "    if len(words) < padding:\n",
    "        words += ([PAD] * (padding - len(words)))\n",
    "    elif len(words) > padding:\n",
    "        words = words[:padding]\n",
    "        \n",
    "#     processed_document = \" \".join(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 818,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "ohe = LabelBinarizer()\n",
    "ohe.fit_transform(data['genre'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_size =  266505\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import spacy\n",
    "\n",
    "class CustomDatasetFromDF(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts = df['lyrics'].values\n",
    "        self.labels = df['genre'].values\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        text = spacy_en.tokenizer(process_document(text))\n",
    "\n",
    "        label = self.labels[index]\n",
    "        label = ohe.transform([label])[0]\n",
    "\n",
    "        return text, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "dataset = CustomDatasetFromDF(data)\n",
    "batch_size = 64\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "print(\"dataset_size = \", len(dataset))\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train = SubsetRandomSampler(train_indices)\n",
    "val = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SubsetRandomSampler' object has no attribute 'sort_key'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-767-41c5d030d4ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBucketIterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/biu-python/lib/python3.6/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36msplits\u001b[0;34m(cls, datasets, batch_sizes, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             ret.append(cls(\n\u001b[0;32m---> 96\u001b[0;31m                 datasets[i], batch_size=batch_sizes[i], train=train, **kwargs))\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/biu-python/lib/python3.6/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, sort_key, device, batch_size_fn, train, repeat, shuffle, sort, sort_within_batch)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_within_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_within_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msort_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SubsetRandomSampler' object has no attribute 'sort_key'"
     ]
    }
   ],
   "source": [
    "train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=64, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 04:51:32: storing 29974x300 projection weights into ./temp.md\n"
     ]
    }
   ],
   "source": [
    "w2v_model.wv.save_word2vec_format(\"./model2.wv.moded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 04:51:42: Loading vectors from ./temp.md.pt\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import Vectors\n",
    "vectors = Vectors(name=\"temp.md\", cache='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "\n",
    "class SongsDataset(data.Dataset):\n",
    "    \n",
    "    name = 'Songs'\n",
    "    df = data\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return len(ex.text)\n",
    "\n",
    "    def __init__(self, df, text_field, label_field, **kwargs):\n",
    "        fields = [(None, None), ('text', text_field), ('label', label_field)]\n",
    "        examples = []\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            text = row['lyrics']\n",
    "            label = row['genre']\n",
    "            examples.append(data.Example.fromlist([text, label], fields))\n",
    "\n",
    "        super(SongsDataset, self).__init__(examples, fields, **kwargs)\n",
    "\n",
    "#     @classmethod\n",
    "#     def splits(cls, text_field, label_field, root='.data',\n",
    "#                train='train', test='test', **kwargs):\n",
    "#         return super(SongsDataset, cls).splits(\n",
    "#             root=root, text_field=text_field, label_field=label_field,\n",
    "#             train=train, validation=None, test=test, **kwargs)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def iters(cls, batch_size=32, device=0, root='.data', vectors=None, **kwargs):\n",
    "        def tokenizer(text): # create a tokenizer function\n",
    "            return [tok.text for tok in spacy_en.tokenizer(process_document(text))]\n",
    "        \n",
    "        TEXT = data.Field(toknizer=tokenizer, fix_length=128)\n",
    "        LABEL = data.Field(sequential=False)\n",
    "\n",
    "        train, test = cls.splits(TEXT, LABEL, root=root, **kwargs)\n",
    "\n",
    "        TEXT.build_vocab(train, vectors=vectors)\n",
    "        LABEL.build_vocab(train)\n",
    "\n",
    "        return data.BucketIterator.splits((train, test), batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Example' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/envs/biu-python/lib/python3.6/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                     \u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/biu-python/lib/python3.6/collections/__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    621\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m                 \u001b[0m_count_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Example' object is not iterable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-762-87fd0ee0b526>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# build the vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mTEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mLABEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/biu-python/lib/python3.6/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m                     \u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         specials = list(OrderedDict.fromkeys(\n\u001b[1;32m    306\u001b[0m             tok for tok in [self.unk_token, self.pad_token, self.init_token,\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Example' object is not iterable"
     ]
    }
   ],
   "source": [
    "# set up fields\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "            return [tok.text for tok in spacy_en.tokenizer(process_document(text))]\n",
    "\n",
    "def one_hot_encoding(label):\n",
    "    vec = ohe.transform([label])[0]\n",
    "    return vec\n",
    "\n",
    "TEXT = data.Field(sequential=True, tokenize=tokenizer, fix_length=128)\n",
    "LABEL = data.Field(sequential=False, preprocessing=one_hot_encoding)\n",
    "\n",
    "# make splits for data\n",
    "# train, test = SongsDataset.splits(TEXT, LABEL)\n",
    "master = SongsDataset(data, TEXT, LABEL)  # your \"master\" dataset\n",
    "n = len(master)  # how many total elements you have\n",
    "n_test = int( n * .2 )  # number of test/val elements\n",
    "n_train = n - 2 * n_test\n",
    "idx = list(range(n))  # indices to all elements\n",
    "random.shuffle(idx)  # in-place shuffle the indices to facilitate random splitting\n",
    "train_idx = idx[:n_train]\n",
    "val_idx = idx[n_train:(n_train + n_test)]\n",
    "test_idx = idx[(n_train + n_test):]\n",
    "\n",
    "train = Subset(master, train_idx)\n",
    "val = Subset(master, val_idx)\n",
    "test = Subset(master, test_idx)\n",
    "\n",
    "# build the vocabulary\n",
    "TEXT.build_vocab(train, vectors=vectors)\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "# make iterator for splits\n",
    "train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=64, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import random\n",
    "import spacy\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    return [tok.text for tok in spacy_en.tokenizer(process_document(text))]\n",
    "\n",
    "def one_hot_encoding(label):\n",
    "    vec = ohe.transform([label])[0]\n",
    "    return vec\n",
    "\n",
    "TEXT = data.Field(sequential=True, tokenize=tokenizer, fix_length=128)\n",
    "LABEL = data.Field(sequential=False, preprocessing=one_hot_encoding)\n",
    "\n",
    "train_data, val_data, test_data = data.TabularDataset.splits(\n",
    "    path='.', \n",
    "    train='train.csv',\n",
    "    validation='val.csv', \n",
    "    test='test.csv', \n",
    "    format='csv',\n",
    "    fields=[(None, None), ('lyrics', TEXT), ('genre', LABEL)],\n",
    "    skip_header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(\n",
    "            tra,\n",
    "            vectors=vectors\n",
    ")\n",
    "LABEL.build_vocab(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Subset' object has no attribute 'sort_key'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-776-2efd23d29e1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train_data, val_data), \n\u001b[0;32m----> 4\u001b[0;31m                                                                            batch_size = BATCH_SIZE)\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/biu-python/lib/python3.6/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36msplits\u001b[0;34m(cls, datasets, batch_sizes, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             ret.append(cls(\n\u001b[0;32m---> 96\u001b[0;31m                 datasets[i], batch_size=batch_sizes[i], train=train, **kwargs))\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/biu-python/lib/python3.6/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, sort_key, device, batch_size_fn, train, repeat, shuffle, sort, sort_within_batch)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_within_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_within_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msort_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Subset' object has no attribute 'sort_key'"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train_data, val_data), \n",
    "                                                                           batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1436,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def process_document(text, remove_panctuations=True, \n",
    "                        remove_stopwords=True, remove_unknown_words=True, padding=128):\n",
    "    \n",
    "    if remove_panctuations:\n",
    "      text = re.sub(\"[^a-zA-Z0-9]\",\" \", text)\n",
    "    #\n",
    "    # Convert words to lower case and split them\n",
    "    words = text.lower().split()\n",
    "    #\n",
    "    # Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w.lower() in stops]  \n",
    "        \n",
    "    if remove_unknown_words:\n",
    "        words = [w for w in words if w in w2v_model.wv]  \n",
    "    \n",
    "    if len(words) < padding:\n",
    "        words += ([PAD] * (padding - len(words)))\n",
    "    elif len(words) > padding:\n",
    "        words = words[:padding]\n",
    "        \n",
    "#     processed_document = \" \".join(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1437,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    return process_document(text) #[tok.text for tok in spacy_en.tokenizer(process_document(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 9, 9, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 1438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "ohe = LabelBinarizer()\n",
    "ohe.fit_transform(data['genre'].values)\n",
    "\n",
    "def one_hot_encoding(label):\n",
    "    vec = ohe.transform([label])[0]\n",
    "    return vec\n",
    "\n",
    "\n",
    "\n",
    "# Import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# creating labelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# Converting string labels into numbers.\n",
    "genres_encoded = le.fit_transform(data[\"genre\"])\n",
    "genres_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 1439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.fit_transform([\"Rock\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1463,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1464,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongsDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pandas.DataFrame): DataFrame object\n",
    "        \"\"\"\n",
    "        self.texts = df['lyrics'].values\n",
    "        self.labels = df['genre'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        text = tokenizer(text)\n",
    "        text = [w2v_model.wv.vocab[word].index for word in text]\n",
    "        label = self.labels[idx]\n",
    "        label = one_hot_encoding(label)\n",
    "\n",
    "        sample = (torch.tensor(text), torch.tensor(label, dtype=torch.float))\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "690"
      ]
     },
     "execution_count": 1465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.vocab['dog'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1466,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SongsDataset(data)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_data, val_data = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   38,   200,   244,   217,   152, 24898,   137,   270,   217,   152,\n",
       "           137,   270,   217,   152,   137,   152,   137,   270,   217,   152,\n",
       "           137,   152,   137,   290,   342,   399,   399,   362,    72,   314,\n",
       "           314,   270,   217,   152,   137,   217,   152,   137,   270,   217,\n",
       "           152,   137,   152,   137, 19839,  7963,  1185,   226,   226,   305,\n",
       "         13469,   270,   140,   140,   270,   217,   152,   137,    91,  1914,\n",
       "           305,   377,    57,    53,   725, 11797,   150,   180,  2624,    66,\n",
       "            57,  1844,   156,    38,   816,   463,    66,    30,   499,   566,\n",
       "            66,   343,  1116,    66,  5091,    38,    48,    66,   170,   290,\n",
       "           631,    52,    48,   139,   725,   150,   301,    66,   967,    38,\n",
       "           967,    66,    91,   290,   131,   150,  3957,    66,   491,   131,\n",
       "            38,    66,    38,    38,   270,    38,    38,   217,    38,   152,\n",
       "            38,   137,    38,   270,   217,   152,   137,   217]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]))"
      ]
     },
     "execution_count": 1467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1468,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    \n",
    "    probs = torch.softmax(preds, dim=1)\n",
    "    winners = probs.argmax(dim=1)\n",
    "    correct = (winners == y.argmax(dim=1)).float() #convert into float for division \n",
    "\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1470,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "#     for batch in iterator:\n",
    "    for index, batch in enumerate(loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch[0]).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch[1])\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch[1])\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1471,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "#         for batch in iterator:\n",
    "        for index, batch in enumerate(loader):\n",
    "\n",
    "            predictions = model(batch[0]).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch[1])\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch[1])\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1472,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1473,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, out_channels, kernel_sizes, dropout, weights, \n",
    "                 freeze_embeddings=True):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        output_size : 2 = (pos, neg)\n",
    "        in_channels : Number of input channels. Here it is 1 as the input data has dimension = (batch_size, num_seq, embedding_length)\n",
    "        output_dim : Number of output channels after convolution operation performed on the input matrix (also number of filters)\n",
    "        kernel_sizes : A list consisting of 3 different kernel_sizes. Convolution will be performed 3 times and finally results from each kernel_size will be concatenated.\n",
    "        dropout : Probability of retaining an activation node during dropout operation\n",
    "        vocab_size : Size of the vocabulary containing unique words\n",
    "        embedding_dim : Embedding dimension of GloVe word embeddings\n",
    "        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table\n",
    "        freeze_embeddings: Boolean flag to unfreeze the word embeddings weights\n",
    "        --------\n",
    "            \n",
    "        \"\"\"\n",
    "        super(CNNClassifier, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_size = output_size\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        \n",
    "        if weights is not None:\n",
    "            print(w2v_model.wv.vectors.shape)\n",
    "            self.word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(weights), freeze=freeze_embeddings)\n",
    "#             self.word_embeddings.weight = nn.Parameter(weights) # using pre-trained weights\n",
    "#             self.word_embeddings.weight.requires_grad=unfreeze_embeddings\n",
    "        else:\n",
    "            self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels = 1,\n",
    "                      out_channels = out_channels,\n",
    "                      kernel_size = (embedding_dim, ks)\n",
    "                     ) for ks in kernel_sizes\n",
    "        ])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(len(kernel_sizes) * out_channels, output_size)\n",
    "\n",
    "    def conv_block(self, input, conv_layer):\n",
    "        conv_out = conv_layer(input) # conv_out.size() = (batch_size, out_channels, dim, 1)\n",
    "        activation = F.relu(conv_out)\n",
    "        activation = activation.squeeze(2) # activation.size() = (batch_size, out_channels, dim1)\n",
    "        max_out = F.max_pool1d(activation, activation.size()[2]).squeeze(2) # maxpool_out.size() = (batch_size, out_channels)\n",
    "        return max_out\n",
    "\n",
    "    def forward(self, input_sentences, batch_size=None):\n",
    "        # input_sentences.size() = (batch_size, num_seq)\n",
    "        embedded = self.word_embeddings(input_sentences)\n",
    "        # embedded.size() = (batch_size, num_seq, embedding_length)\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        # embedded.size() = (batch_size, 1, num_seq, embedding_length)\n",
    "        embedded = embedded.permute(0, 1, 3, 2)\n",
    "        # input.size() = (batch_size, 1, embedding_length, num_seq)\n",
    "        max_out = [self.conv_block(embedded, conv) for conv in self.convs]\n",
    "        #max_out = [batch size, out_channels]\n",
    "        all_out = torch.cat(max_out, dim=1)\n",
    "        # all_out.size() = (batch_size, num_kernels*out_channels)\n",
    "        fc_in = self.dropout(all_out)\n",
    "        # fc_in.size()) = (batch_size, num_kernels*out_channels)\n",
    "        logits = self.fc(fc_in)\n",
    "        # logits.size() = (batch_size, output_size)\n",
    "\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate an instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29974, 300)"
      ]
     },
     "execution_count": 1474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = w2v_model.wv.vectors\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29974, 300])"
      ]
     },
     "execution_count": 1475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor(w2v_model.wv.vectors).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1476,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ae1n8h6eJev"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29974, 300)\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 12 # 12 genres of songs\n",
    "out_channels = 100\n",
    "kernel_sizes = [3, 4, 5]\n",
    "DROPOUT = 0.25\n",
    "VOCAB_SIZE = len(w2v_model.wv.vocab)\n",
    "EMBED_DIM = 300\n",
    "weights =  w2v_model.wv.vectors #TEXT.vocab.vectors\n",
    "\n",
    "model = CNNClassifier(vocab_size=VOCAB_SIZE, embedding_dim=EMBED_DIM, output_dim=NUM_CLASSES, \n",
    "                      out_channels=out_channels, kernel_sizes=kernel_sizes, dropout=DROPOUT, \n",
    "                      weights=weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split dataset and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 61m 39s\n",
      "\tTrain Loss: 0.197 | Train Acc: 50.88%\n",
      "\t Val. Loss: 0.187 |  Val. Acc: 53.08%\n",
      "Epoch: 02 | Epoch Time: 68m 20s\n",
      "\tTrain Loss: 0.185 | Train Acc: 53.68%\n",
      "\t Val. Loss: 0.183 |  Val. Acc: 54.19%\n",
      "Epoch: 03 | Epoch Time: 81m 59s\n",
      "\tTrain Loss: 0.180 | Train Acc: 55.20%\n",
      "\t Val. Loss: 0.181 |  Val. Acc: 54.69%\n",
      "Epoch: 04 | Epoch Time: 114m 31s\n",
      "\tTrain Loss: 0.175 | Train Acc: 56.44%\n",
      "\t Val. Loss: 0.180 |  Val. Acc: 54.83%\n",
      "Epoch: 05 | Epoch Time: 110m 44s\n",
      "\tTrain Loss: 0.171 | Train Acc: 57.90%\n",
      "\t Val. Loss: 0.180 |  Val. Acc: 55.47%\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "N_EPOCHS = 5\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "all_train_losses = []\n",
    "all_val_losses = []\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    all_train_losses.append(train_loss)\n",
    "    train_accuracy.append(train_acc)\n",
    "\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    all_val_losses.append(val_loss)\n",
    "    val_accuracy.append(val_acc)\n",
    "    \n",
    "    end_time = time.tisme()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'lyrics-best-cnn-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. Acc: {val_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_train_losses, 'r')\n",
    "plt.plot(all_val_losses, 'g')\n",
    "plt.plot(train_accuracy, 'b')\n",
    "plt.plot(val_accuracy, 'y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "DL_word_embedding_assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:biu-python] *",
   "language": "python",
   "name": "conda-env-biu-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
