{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN for text classification and text generation\n",
    "### Dr. Omri Allouche 2018. YData Deep Learning Course\n",
    "\n",
    "[Open in Google Colab](https://colab.research.google.com/github/omriallouche/deep_learning_course/blob/master/DL_rnn_text_classification_generation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of this exercise, we’ll continue our attempts to classify text using different network architectures. This time, we’ll try a LSTM. We'll use the Metrolyrics dataset we used in the previous exercise.  \n",
    "\n",
    "You are encouraged to review the code in [this](https://github.com/prakashpandey9/Text-Classification-Pytorch) repo, that contains implementation of several deep learning architectures for text classification in PyTorch. If you face time limitations, you're welcome to adapt it to your needs instead of writing your own code from scratch.\n",
    "\n",
    "In the second part of this exercise, you'll unleash the hidden creativity of your computer, by letting it generate Country songs (yeehaw!). You'll train a character-level RNN-based language model, and use it to generate new songs.\n",
    "\n",
    "\n",
    "### Special Note\n",
    "Our Deep Learning course was packed with both theory and practice. In a short time, you've got to learn the basics of deep learning theory and get hands-on experience training and using pretrained DL networks, while learning PyTorch.  \n",
    "Past exercises required a lot of work, and hopefully gave you a sense of the challenges and difficulties one faces when using deep learning in the real world. While the investment you've made in the course so far is enormous, I strongly encourage you to take a stab at this exercise. \n",
    "\n",
    "DL networks for NLP are much shallower than those for image classification. It's possible to construct your own networks from scratch, and achieve nice results. While I hope the theoretical foundations of RNNs are clear after our class sessions, getting your hands dirty with their implementation in PyTorch allows you to set breakpoints, watch the dimensions of the different layers and components and get a much better understand of theory, in addition to code that might prove useful later for your own projects. \n",
    "\n",
    "I tried to provide references for all parts that walk you through a very similar task (actually, the same task on a different dataset). I expect this exercise to require much less of your time than previous exercises.\n",
    "\n",
    "The exercise is aimed to help you get better understanding of the concepts. I am not looking for the optimal model performance, and don't look for extensive optimization of hyperparameters. The task we face in this exercise, namely the classification of the song’s genre from its text alone, is quite challenging, and we probably shouldn’t expect great results from our classifier. Don’t let this discourage you - not every task reaches an f1 score of 90%+. \n",
    "\n",
    "In fact, some of the reasons I chose this dataset is because it highlights some of the issues we face in machine learning models in the real world. Examples include:\n",
    "- The classes are highly imbalanced - try to think how this affects the network learning\n",
    "- Given the small amount of data for some classes, you might actually prefer to remove them from the dataset. How would you decide that?\n",
    "- NLP tasks often involve preprocessing (lowercasing, tokenization, lemmatization, stopwords removal etc.). The decision on the actual preprocessing pipeline depends on the task, and is often influenced by our believes about the data and exploratory analysis of it. Thinking conciously about these questions helps you be a better data scientist\n",
    "- Some songs contain no lyrics (for example, they just contain the text \"instrumental\"). Others include non-English characters. You'll often need to preprocess your data and make decisions as to what your network should actually get as input (think - how should you treat newline characters?)\n",
    "- While model performance on this dataset are not amazing, we can try to answer interesting follow-up questions - which genres are more similar to each other and are often confused? Do genres become more similar through the years? ...\n",
    "\n",
    "More issues will probably pop up while you're working on this task. If you face technical difficulties or find a step in the process that takes too long, please let me know. It would also be great if you share with the class code you wrote that speeds up some of the work (for example, a data loader class, a parsed dataset etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN for Text Classification\n",
    "In this section you'll write a text classifier using LSTM, to determine the genre of a song based on its lyrics.  \n",
    "The code needed for this section should be very similar to code you've written for the previous exercise, and use the same dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "raw_data = pd.read_csv(\"380000-lyrics-from-metrolyrics.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(362237, 6)\n",
      "(332423, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>song</th>\n",
       "      <th>year</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ego-remix</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Oh baby, how you doing?\\nYou know I'm gonna cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>then-tell-me</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>playin' everything so easy,\\nit's like you see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>honesty</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>If you search\\nFor tenderness\\nIt isn't hard t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>you-are-my-rock</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Oh oh oh I, oh oh oh I\\n[Verse 1:]\\nIf I wrote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>black-culture</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Party the people, the people the party it's po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362232</td>\n",
       "      <td>362232</td>\n",
       "      <td>who-am-i-drinking-tonight</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>I gotta say\\nBoy, after only just a couple of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362233</td>\n",
       "      <td>362233</td>\n",
       "      <td>liar</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>I helped you find her diamond ring\\nYou made m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362234</td>\n",
       "      <td>362234</td>\n",
       "      <td>last-supper</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>Look at the couple in the corner booth\\nLooks ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362235</td>\n",
       "      <td>362235</td>\n",
       "      <td>christ-alone-live-in-studio</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>When I fly off this mortal earth\\nAnd I'm meas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362236</td>\n",
       "      <td>362236</td>\n",
       "      <td>amen</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>I heard from a friend of a friend of a friend ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>332423 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index                         song  year           artist    genre  \\\n",
       "0            0                    ego-remix  2009  beyonce-knowles      Pop   \n",
       "1            1                 then-tell-me  2009  beyonce-knowles      Pop   \n",
       "2            2                      honesty  2009  beyonce-knowles      Pop   \n",
       "3            3              you-are-my-rock  2009  beyonce-knowles      Pop   \n",
       "4            4                black-culture  2009  beyonce-knowles      Pop   \n",
       "...        ...                          ...   ...              ...      ...   \n",
       "362232  362232    who-am-i-drinking-tonight  2012       edens-edge  Country   \n",
       "362233  362233                         liar  2012       edens-edge  Country   \n",
       "362234  362234                  last-supper  2012       edens-edge  Country   \n",
       "362235  362235  christ-alone-live-in-studio  2012       edens-edge  Country   \n",
       "362236  362236                         amen  2012       edens-edge  Country   \n",
       "\n",
       "                                                   lyrics  \n",
       "0       Oh baby, how you doing?\\nYou know I'm gonna cu...  \n",
       "1       playin' everything so easy,\\nit's like you see...  \n",
       "2       If you search\\nFor tenderness\\nIt isn't hard t...  \n",
       "3       Oh oh oh I, oh oh oh I\\n[Verse 1:]\\nIf I wrote...  \n",
       "4       Party the people, the people the party it's po...  \n",
       "...                                                   ...  \n",
       "362232  I gotta say\\nBoy, after only just a couple of ...  \n",
       "362233  I helped you find her diamond ring\\nYou made m...  \n",
       "362234  Look at the couple in the corner booth\\nLooks ...  \n",
       "362235  When I fly off this mortal earth\\nAnd I'm meas...  \n",
       "362236  I heard from a friend of a friend of a friend ...  \n",
       "\n",
       "[332423 rows x 6 columns]"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the data\n",
    "data = raw_data\n",
    "print(data.shape)\n",
    "data = data.loc[data[\"lyrics\"].str.len() > 3] # filter for songs with more than 3 words\n",
    "data = raw_data[raw_data[\"genre\"] != \"Not Available\"] # remove \"Not Available genre\"\n",
    "data = data[data[\"genre\"].notnull()] # remove null genres\n",
    "print(data.shape)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suffle the data\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "data = shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rock          131377\n",
       "Pop            49444\n",
       "Hip-Hop        33965\n",
       "Metal          28408\n",
       "Other          23683\n",
       "Country        17286\n",
       "Jazz           17147\n",
       "Electronic     16205\n",
       "R&B             5935\n",
       "Indie           5732\n",
       "Folk            3241\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres_distribution = data['genre'].value_counts()\n",
    "genres_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12ce00c88>"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "genres_distribution.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Songs with no lyrics is 95680 / 362237 = 0.264136463144295\n",
      "Instrumental songs is 80 / 362237 0.00022084988557215303%\n"
     ]
    }
   ],
   "source": [
    "data_missing_lyrics = raw_data[raw_data['lyrics'].isnull()]\n",
    "print(f\"Songs with no lyrics is {len(data_missing_lyrics)} / {len(raw_data['lyrics'])} = {len(data_missing_lyrics) / len(raw_data['lyrics'])}\")\n",
    "      \n",
    "data_instrumental_lyrics = raw_data[raw_data['lyrics'] == \"instrumental\"]\n",
    "print(f\"Instrumental songs is {len(data_instrumental_lyrics)} / {len(raw_data['lyrics'])} {len(data_instrumental_lyrics) / len(raw_data['lyrics'])}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['lyrics'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['lyrics'] != \"Not Available\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:42:51: loading Word2Vec object from 300features_40minwords_10context.wv.model\n",
      "INFO - 22:42:51: loading wv recursively from 300features_40minwords_10context.wv.model.wv.* with mmap=None\n",
      "INFO - 22:42:51: setting ignored attribute vectors_norm to None\n",
      "INFO - 22:42:51: loading vocabulary recursively from 300features_40minwords_10context.wv.model.vocabulary.* with mmap=None\n",
      "INFO - 22:42:51: loading trainables recursively from 300features_40minwords_10context.wv.model.trainables.* with mmap=None\n",
      "INFO - 22:42:51: setting ignored attribute cum_table to None\n",
      "INFO - 22:42:51: loaded 300features_40minwords_10context.wv.model\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_model = Word2Vec.load(\"300features_40minwords_10context.wv.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "PAD = \"<pad>\"\n",
    "w2v_model.wv[\"<pad>\"] = np.zeros(300,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def process_document(text, remove_panctuations=True, \n",
    "                        remove_stopwords=True, remove_unknown_words=True, fix_length=200):\n",
    "    \n",
    "    if remove_panctuations:\n",
    "      text = re.sub(\"[^a-zA-Z0-9]\",\" \", text)\n",
    "    #\n",
    "    # Convert words to lower case and split them\n",
    "    words = text.lower().split()\n",
    "    #\n",
    "    # Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w.lower() in stops]  \n",
    "        \n",
    "    if remove_unknown_words:\n",
    "        words = [w for w in words if w in w2v_model.wv]\n",
    "        \n",
    "    if len(words) < fix_length:\n",
    "        words += ([PAD] * (fix_length - len(words)))\n",
    "    elif len(words) > fix_length:\n",
    "        words = words[:fix_length]\n",
    "        \n",
    "#     processed_document = \" \".join(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text): # create a tokenizer function\n",
    "    return process_document(text) #[tok.text for tok in spacy_en.tokenizer(process_document(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 9, 9, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "ohe = LabelBinarizer()\n",
    "ohe.fit_transform(valid_data['genre'].values)\n",
    "\n",
    "def one_hot_encoding(label):\n",
    "    vec = ohe.transform([label])[0]\n",
    "    return vec\n",
    "\n",
    "\n",
    "\n",
    "# Import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# creating labelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# Converting string labels into numbers.\n",
    "genres_encoded = le.fit_transform(valid_data[\"genre\"])\n",
    "genres_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongsDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pandas.DataFrame): DataFrame object\n",
    "        \"\"\"\n",
    "        self.texts = df['lyrics'].values\n",
    "        self.labels = df['genre'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        text = tokenizer(text)\n",
    "        text = [w2v_model.wv.vocab[word].index for word in text]\n",
    "        label = self.labels[idx]\n",
    "        label = one_hot_encoding(label)\n",
    "\n",
    "        sample = (torch.tensor(text), torch.tensor(label))\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  992,    96,   224,  7629,   137,   144,   918,  4956,    81,   918,\n",
       "          1945,    95,    50,  1327,    62,    50,    53,    88,   771,   128,\n",
       "           280,   245,   280,  1945,    95,    62,   245,   171,    53,   512,\n",
       "          1797,   234,    57,   148,   651,   280,   245,   280, 29972, 29972,\n",
       "         29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972,\n",
       "         29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972,\n",
       "         29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972,\n",
       "         29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972,\n",
       "         29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972,\n",
       "         29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972,\n",
       "         29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972,\n",
       "         29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972,\n",
       "         29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972,\n",
       "         29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972,\n",
       "         29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972,\n",
       "         29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972,\n",
       "         29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972,\n",
       "         29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972,\n",
       "         29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972,\n",
       "         29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]))"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = SongsDataset(data[:1000])\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_data, val_data = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=8, shuffle=True)\n",
    "\n",
    "dataset.__getitem__(99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights, freeze_embeddings=False):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "        output_size : 2 = (pos, neg)\n",
    "        hidden_sie : Size of the hidden_state of the LSTM\n",
    "        vocab_size : Size of the vocabulary containing unique words\n",
    "        embedding_length : Embeddding dimension of GloVe word embeddings\n",
    "        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_length = embedding_length\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(weights), freeze=freeze_embeddings)\n",
    "#         self.word_embeddings = nn.Embedding(vocab_size, embedding_length)# Initializing the look-up table.\n",
    "#         self.word_embeddings.weight = nn.Parameter(weights, requires_grad=False) # Assigning the look-up table to the pre-trained GloVe word embedding.\n",
    "        self.lstm = nn.LSTM(embedding_length, hidden_size)\n",
    "        self.dense = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_sentence, batch_size=None):\n",
    "\n",
    "        \"\"\" \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
    "        batch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Output of the linear layer containing logits for positive & negative class which receives its input as the final_hidden_state of the LSTM\n",
    "        final_output.shape = (batch_size, output_size)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ''' Here we will map all the indexes present in the input sequence to the corresponding word vector using our pre-trained word_embedddins.'''\n",
    "        input = self.word_embeddings(input_sentence) # embedded input of shape = (batch_size, num_sequences,  embedding_length)\n",
    "        input = input.permute(1, 0, 2) # input.size() = (num_sequences, batch_size, embedding_length)\n",
    "\n",
    "        if batch_size is None:\n",
    "            h_0 = torch.zeros(1, self.batch_size, self.hidden_size) # Initial hidden state of the LSTM\n",
    "            c_0 = torch.zeros(1, self.batch_size, self.hidden_size) # Initial cell state of the LSTM\n",
    "        else:\n",
    "            h_0 = torch.zeros(1, batch_size, self.hidden_size)\n",
    "            c_0 = torch.zeros(1, batch_size, self.hidden_size)\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n",
    "        logits = self.dense(final_hidden_state[-1]) # final_hidden_state.size() = (1, batch_size, hidden_size) & final_output.size() = (batch_size, output_size)\n",
    "    \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 11 # 11 genres of songs\n",
    "DROPOUT = 0.25\n",
    "VOCAB_SIZE = len(w2v_model.wv.vocab)\n",
    "EMBED_DIM = 300\n",
    "HIDDEN_SIZE=256\n",
    "BATCH_SIZE=8\n",
    "weights =  w2v_model.wv.vectors #TEXT.vocab.vectors\n",
    "\n",
    "model = LSTMClassifier(batch_size=BATCH_SIZE, output_size=NUM_CLASSES, hidden_size=HIDDEN_SIZE, \n",
    "                       vocab_size=VOCAB_SIZE, embedding_length=EMBED_DIM, weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    \n",
    "    probs = torch.softmax(preds, dim=1)\n",
    "    winners = probs.argmax(dim=1)\n",
    "    correct = (winners == y.argmax(dim=1)).float() #convert into float for division \n",
    "\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for index, batch in enumerate(loader):\n",
    "        \n",
    "        # handle case where one of the batches is of size that is smaller than declared batch size\n",
    "        # todo!!!\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch[0]).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch[1])\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch[1])\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for index, batch in enumerate(loader):\n",
    "\n",
    "            predictions = model(batch[0]).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch[1])\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch[1])\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)\n",
    "\n",
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (1, 16, 256), got (1, 8, 256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-658-7f7babe34ed4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mall_train_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-657-4fcc6ae04c84>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/biu-python/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-655-146aa1005699>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_sentence, batch_size)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mh_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mc_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_hidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_cell_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# final_hidden_state.size() = (1, batch_size, hidden_size) & final_output.size() = (batch_size, output_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/biu-python/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/biu-python/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/biu-python/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/biu-python/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n",
      "\u001b[0;32m/anaconda3/envs/biu-python/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         self.check_hidden_size(hidden[0], expected_hidden_size,\n\u001b[0;32m--> 500\u001b[0;31m                                'Expected hidden[0] size {}, got {}')\n\u001b[0m\u001b[1;32m    501\u001b[0m         self.check_hidden_size(hidden[1], expected_hidden_size,\n\u001b[1;32m    502\u001b[0m                                'Expected hidden[1] size {}, got {}')\n",
      "\u001b[0;32m/anaconda3/envs/biu-python/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;31m# type: (Tensor, Tuple[int, int, int], str) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden[0] size (1, 16, 256), got (1, 8, 256)"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "all_train_losses = []\n",
    "all_val_losses = []\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    all_train_losses.append(train_loss)\n",
    "    train_accuracy.append(train_acc)\n",
    "\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    all_val_losses.append(val_loss)\n",
    "    val_accuracy.append(val_acc)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best-model.model')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. Acc: {val_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_train_losses, 'r')\n",
    "plt.plot(all_val_losses, 'g')\n",
    "plt.plot(train_accuracy, 'b')\n",
    "plt.plot(val_accuracy, 'y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN for Text Generation\n",
    "In this section, we'll use an LSTM to generate new songs. You can pick any genre you like, or just use all genres. You can even try to generate songs in the style of a certain artist - remember that the Metrolyrics dataset contains the author of each song. \n",
    "\n",
    "For this, we’ll first train a character-based language model. We’ve mostly discussed in class the usage of RNNs to predict the next word given past words, but as we’ve mentioned in class, RNNs can also be used to learn sequences of characters.\n",
    "\n",
    "First, please go through the [PyTorch tutorial](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html) on generating family names. You can download a .py file or a jupyter notebook with the entire code of the tutorial. \n",
    "\n",
    "As a reminder of topics we've discussed in class, see Andrej Karpathy's popular blog post [\"The Unreasonable Effectiveness of Recurrent Neural Networks\"](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). You are also encouraged to view [this](https://gist.github.com/karpathy/d4dee566867f8291f086) vanilla implementation of a character-level RNN, written in numpy with just 100 lines of code, including the forward and backward passes.  \n",
    "\n",
    "Other tutorials that might prove useful:\n",
    "1. http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/\n",
    "1. https://github.com/mcleonard/pytorch-charRNN\n",
    "1. https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from csv\n",
    "import pandas as pd\n",
    "\n",
    "raw_data = pd.read_csv(\"380000-lyrics-from-metrolyrics.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   lyrics    genre\n",
      "0       Oh baby, how you doing?\\nYou know I'm gonna cu...      Pop\n",
      "1       playin' everything so easy,\\nit's like you see...      Pop\n",
      "2       If you search\\nFor tenderness\\nIt isn't hard t...      Pop\n",
      "3       Oh oh oh I, oh oh oh I\\n[Verse 1:]\\nIf I wrote...      Pop\n",
      "4       Party the people, the people the party it's po...      Pop\n",
      "...                                                   ...      ...\n",
      "362232  I gotta say\\nBoy, after only just a couple of ...  Country\n",
      "362233  I helped you find her diamond ring\\nYou made m...  Country\n",
      "362234  Look at the couple in the corner booth\\nLooks ...  Country\n",
      "362235  When I fly off this mortal earth\\nAnd I'm meas...  Country\n",
      "362236  I heard from a friend of a friend of a friend ...  Country\n",
      "\n",
      "[362237 rows x 2 columns]\n",
      "                                                   lyrics genre\n",
      "0       Oh baby, how you doing?\\nYou know I'm gonna cu...   Pop\n",
      "1       playin' everything so easy,\\nit's like you see...   Pop\n",
      "2       If you search\\nFor tenderness\\nIt isn't hard t...   Pop\n",
      "3       Oh oh oh I, oh oh oh I\\n[Verse 1:]\\nIf I wrote...   Pop\n",
      "4       Party the people, the people the party it's po...   Pop\n",
      "...                                                   ...   ...\n",
      "362210  When the photographs you're taking now\\nAre ta...   Pop\n",
      "362211  I met Moko jumbie,\\nHe walks on stilts through...   Pop\n",
      "362212  Chill on the hollow ponds\\nSet sail by a kid\\n...   Pop\n",
      "362213  Celebrate the passing drugs\\nPut them on the b...   Pop\n",
      "362214  When the serve is done\\nAnd the parish shuffle...   Pop\n",
      "\n",
      "[49444 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# let's see the data: browse through different songe genres:\n",
    "print(raw_data[['lyrics', 'genre']])\n",
    "\n",
    "print(raw_data[raw_data['genre'] == 'Pop'][['lyrics', 'genre']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362237"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_data.dropna(subset=['genre']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362237"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x15530ae48>"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "genres_distribution = data['genre'].value_counts()\n",
    "genres_distribution.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         384\n",
       "1         220\n",
       "2         116\n",
       "3         436\n",
       "4         280\n",
       "         ... \n",
       "362232    280\n",
       "362233    175\n",
       "362234    213\n",
       "362235    145\n",
       "362236    287\n",
       "Name: lyrics, Length: 266556, dtype: int64"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(raw_data.dropna())['lyrics'].apply(lambda lyrics: len(lyrics.split(\" \")) if lyrics is not None else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:56:04: loading Word2Vec object from 300features_40minwords_10context.wv.model\n",
      "INFO - 22:56:05: loading wv recursively from 300features_40minwords_10context.wv.model.wv.* with mmap=None\n",
      "INFO - 22:56:05: setting ignored attribute vectors_norm to None\n",
      "INFO - 22:56:05: loading vocabulary recursively from 300features_40minwords_10context.wv.model.vocabulary.* with mmap=None\n",
      "INFO - 22:56:05: loading trainables recursively from 300features_40minwords_10context.wv.model.trainables.* with mmap=None\n",
      "INFO - 22:56:05: setting ignored attribute cum_table to None\n",
      "INFO - 22:56:05: loaded 300features_40minwords_10context.wv.model\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# load the w2v model\n",
    "w2v_model = Word2Vec.load(\"300features_40minwords_10context.wv.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=16\n",
    "VOCAB_SIZE = len(w2v_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(text, remove_panctuations=True, \n",
    "                        remove_stopwords=True, remove_unknown_words=True, fix_length=200):\n",
    "    \n",
    "    if remove_panctuations:\n",
    "      text = re.sub(\"[^a-zA-Z0-9]\",\" \", text)\n",
    "    #\n",
    "    # Convert words to lower case and split them\n",
    "    words = text.lower().split()\n",
    "    #\n",
    "    # Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w.lower() in stops]  \n",
    "        \n",
    "    if remove_unknown_words:\n",
    "        words = [w for w in words if w in w2v_model.wv]\n",
    "        \n",
    "    if len(words) < fix_length:\n",
    "        words += ([PAD] * (fix_length - len(words)))\n",
    "    elif len(words) > fix_length:\n",
    "        words = words[:fix_length]\n",
    "        \n",
    "#     processed_document = \" \".join(words)\n",
    "    return words\n",
    "\n",
    "def tokenizer(text, **kwargs): # create a tokenizer function\n",
    "    return process_document(text, **kwargs) #[tok.text for tok in spacy_en.tokenizer(process_document(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class SongsGenerationDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, df, batch_size, vocab_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pandas.DataFrame): DataFrame object\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        lyrics = df['lyrics'].values\n",
    "        lyrics = [tokenizer(text, fix_length=-1) for text in lyrics]\n",
    "        lyrics = [word for words in lyrics for word in words] # flatten list\n",
    "        # Next step, we will convert word tokens into integer indices. \n",
    "        # These will be the input to the network. And because we will train a mini-batch each iteration, \n",
    "        # we should be able to split the data into batches evenly. \n",
    "        # We can assure that by chopping out the last uneven batch\n",
    "        lyrics = [w2v_model.wv.vocab[word].index for word in lyrics]\n",
    "        lyrics = lyrics[:(batch_size * (int)(len(lyrics) / batch_size))]\n",
    "        \n",
    "        self.words_in = lyrics\n",
    "        self.words_out = np.zeros_like(self.words_in)\n",
    "        self.words_out[:-1] = self.words_in[1:]\n",
    "        self.words_out[-1] = self.words_in[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words_in)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.words_in[idx]\n",
    "#         label = np.zeros(self.vocab_size)\n",
    "#         label[self.words_out[idx]] = 1\n",
    "        label = self.words_out[idx]\n",
    "\n",
    "        sample = (torch.tensor(text), torch.tensor(label))\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SongsGenerationDataset(data[data[\"genre\"] == \"Hip-Hop\"][:1000], BATCH_SIZE, VOCAB_SIZE)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_data, val_data = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3346), tensor(304))"
      ]
     },
     "execution_count": 913,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM2(nn.Module):\n",
    "    def __init__(self, n_vocab, embedding_size, hidden_size, weights, batch_size, freeze_embeddings=True):\n",
    "        super(LSTM2, self).__init__()\n",
    "    \n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = 1\n",
    "        \n",
    "        if weights is not None:\n",
    "            self.word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(weights), freeze=freeze_embeddings)\n",
    "        else:\n",
    "            self.word_embeddings = nn.Embedding(n_vocab, embedding_size)\n",
    "            \n",
    "        self.lstm = nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=self.num_layers)\n",
    "    \n",
    "        self.dense = nn.Linear(hidden_size, n_vocab)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.word_embeddings(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        output = self.dense(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, state\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_size),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a model\n",
    "VOCAB_SIZE = len(w2v_model.wv.vocab)\n",
    "EMBED_DIM = 300\n",
    "HIDDEN_SIZE=32\n",
    "weights =  w2v_model.wv.vectors #TEXT.vocab.vectors\n",
    "\n",
    "model = LSTM2(n_vocab=VOCAB_SIZE, embedding_size=EMBED_DIM, hidden_size=HIDDEN_DIM, batch_size=BATCH_SIZE, weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)\n",
    "        \n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    \n",
    "    winners = preds.argmax(dim=1)\n",
    "    correct = (winners == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    state_h, state_c = model.init_hidden()\n",
    "    \n",
    "    for index, (x, y) in enumerate(loader):\n",
    "        x = x.unsqueeze(0)\n",
    "        y = torch.autograd.Variable(y).long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "        predictions = torch.squeeze(predictions, 0)\n",
    "        \n",
    "        loss = criterion(predictions, y)\n",
    "        acc = binary_accuracy(predictions, y)\n",
    "        \n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    state_h, state_c = model.init_hidden()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for index, (x, y) in enumerate(loader):\n",
    "            x = x.unsqueeze(0)\n",
    "                \n",
    "            predictions, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            predictions = torch.squeeze(predictions, 0)\n",
    "            \n",
    "            loss = criterion(predictions, y)\n",
    "            acc = binary_accuracy(predictions, y)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-914-e8c3a3349bd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mall_train_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-910-8bf8a50378cc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/biu-python/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/biu-python/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "N_EPOCHS = 5\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "all_train_losses = []\n",
    "all_val_losses = []\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    all_train_losses.append(train_loss)\n",
    "    train_accuracy.append(train_acc)\n",
    "\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    all_val_losses.append(val_loss)\n",
    "    val_accuracy.append(val_acc)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'lstm-gen-best-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. Acc: {val_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(device, model, length, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
    "    model.eval()\n",
    "\n",
    "    state_h, state_c = model.init_hidden((1))\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    \n",
    "    for _ in range(length):\n",
    "        index = torch.tensor([[w2v_model.wv.vocab[w].index]).to(device)\n",
    "        output, (state_h, state_c) = model(index, (state_h, state_c))\n",
    "    \n",
    "        _, top_index = torch.topk(output[0], k=top_k)\n",
    "        choices = top_index.tolist()\n",
    "        choice = np.random.choice(choices[0])\n",
    "\n",
    "        words.append(w2v_model.wv.vocab[choice].index)\n",
    "\n",
    "    print(' '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Tips\n",
    "As a final tip, I do encourage you to do most of the work first on your local machine. They say that Data Scientists spend 80% of their time cleaning the data and preparing it for training (and 20% complaining about cleaning the data and preparing it). Handling these parts on your local machine usually mean you will spend less time complaining. You can switch to the cloud once your code runs and your pipeline is in place, for the actual training using a GPU.  \n",
    "\n",
    "I also encourage you to use a small subset of the dataset first, so things run smoothly. The Metrolyrics dataset contains over 300k songs. You can start with a much much smaller set (even 3,000 songs) and try to train a network based on it. Once everything runs properly, add more data. \n",
    "\n",
    "Good luck!  \n",
    "Omri"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:biu-python] *",
   "language": "python",
   "name": "conda-env-biu-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
