{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "anitaostroumov@gmail.com אניטה אוסטרואומוב  310500160\n",
    "asaf.ach@gmail.com אסף אחי מרדכי 035892306"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN for text classification and text generation\n",
    "### Dr. Omri Allouche 2018. YData Deep Learning Course\n",
    "\n",
    "[Open in Google Colab](https://colab.research.google.com/github/omriallouche/deep_learning_course/blob/master/DL_rnn_text_classification_generation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of this exercise, we’ll continue our attempts to classify text using different network architectures. This time, we’ll try a LSTM. We'll use the Metrolyrics dataset we used in the previous exercise.  \n",
    "\n",
    "You are encouraged to review the code in [this](https://github.com/prakashpandey9/Text-Classification-Pytorch) repo, that contains implementation of several deep learning architectures for text classification in PyTorch. If you face time limitations, you're welcome to adapt it to your needs instead of writing your own code from scratch.\n",
    "\n",
    "In the second part of this exercise, you'll unleash the hidden creativity of your computer, by letting it generate Country songs (yeehaw!). You'll train a character-level RNN-based language model, and use it to generate new songs.\n",
    "\n",
    "\n",
    "### Special Note\n",
    "Our Deep Learning course was packed with both theory and practice. In a short time, you've got to learn the basics of deep learning theory and get hands-on experience training and using pretrained DL networks, while learning PyTorch.  \n",
    "Past exercises required a lot of work, and hopefully gave you a sense of the challenges and difficulties one faces when using deep learning in the real world. While the investment you've made in the course so far is enormous, I strongly encourage you to take a stab at this exercise. \n",
    "\n",
    "DL networks for NLP are much shallower than those for image classification. It's possible to construct your own networks from scratch, and achieve nice results. While I hope the theoretical foundations of RNNs are clear after our class sessions, getting your hands dirty with their implementation in PyTorch allows you to set breakpoints, watch the dimensions of the different layers and components and get a much better understand of theory, in addition to code that might prove useful later for your own projects. \n",
    "\n",
    "I tried to provide references for all parts that walk you through a very similar task (actually, the same task on a different dataset). I expect this exercise to require much less of your time than previous exercises.\n",
    "\n",
    "The exercise is aimed to help you get better understanding of the concepts. I am not looking for the optimal model performance, and don't look for extensive optimization of hyperparameters. The task we face in this exercise, namely the classification of the song’s genre from its text alone, is quite challenging, and we probably shouldn’t expect great results from our classifier. Don’t let this discourage you - not every task reaches an f1 score of 90%+. \n",
    "\n",
    "In fact, some of the reasons I chose this dataset is because it highlights some of the issues we face in machine learning models in the real world. Examples include:\n",
    "- The classes are highly imbalanced - try to think how this affects the network learning\n",
    "- Given the small amount of data for some classes, you might actually prefer to remove them from the dataset. How would you decide that?\n",
    "- NLP tasks often involve preprocessing (lowercasing, tokenization, lemmatization, stopwords removal etc.). The decision on the actual preprocessing pipeline depends on the task, and is often influenced by our believes about the data and exploratory analysis of it. Thinking conciously about these questions helps you be a better data scientist\n",
    "- Some songs contain no lyrics (for example, they just contain the text \"instrumental\"). Others include non-English characters. You'll often need to preprocess your data and make decisions as to what your network should actually get as input (think - how should you treat newline characters?)\n",
    "- While model performance on this dataset are not amazing, we can try to answer interesting follow-up questions - which genres are more similar to each other and are often confused? Do genres become more similar through the years? ...\n",
    "\n",
    "More issues will probably pop up while you're working on this task. If you face technical difficulties or find a step in the process that takes too long, please let me know. It would also be great if you share with the class code you wrote that speeds up some of the work (for example, a data loader class, a parsed dataset etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN for Text Classification\n",
    "In this section you'll write a text classifier using LSTM, to determine the genre of a song based on its lyrics.  \n",
    "The code needed for this section should be very similar to code you've written for the previous exercise, and use the same dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "raw_data = pd.read_csv(\"380000-lyrics-from-metrolyrics.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(362237, 6)\n",
      "(332423, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>song</th>\n",
       "      <th>year</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ego-remix</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Oh baby, how you doing?\\nYou know I'm gonna cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>then-tell-me</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>playin' everything so easy,\\nit's like you see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>honesty</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>If you search\\nFor tenderness\\nIt isn't hard t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>you-are-my-rock</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Oh oh oh I, oh oh oh I\\n[Verse 1:]\\nIf I wrote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>black-culture</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Party the people, the people the party it's po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362232</td>\n",
       "      <td>362232</td>\n",
       "      <td>who-am-i-drinking-tonight</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>I gotta say\\nBoy, after only just a couple of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362233</td>\n",
       "      <td>362233</td>\n",
       "      <td>liar</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>I helped you find her diamond ring\\nYou made m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362234</td>\n",
       "      <td>362234</td>\n",
       "      <td>last-supper</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>Look at the couple in the corner booth\\nLooks ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362235</td>\n",
       "      <td>362235</td>\n",
       "      <td>christ-alone-live-in-studio</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>When I fly off this mortal earth\\nAnd I'm meas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362236</td>\n",
       "      <td>362236</td>\n",
       "      <td>amen</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>I heard from a friend of a friend of a friend ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>332423 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index                         song  year           artist    genre  \\\n",
       "0            0                    ego-remix  2009  beyonce-knowles      Pop   \n",
       "1            1                 then-tell-me  2009  beyonce-knowles      Pop   \n",
       "2            2                      honesty  2009  beyonce-knowles      Pop   \n",
       "3            3              you-are-my-rock  2009  beyonce-knowles      Pop   \n",
       "4            4                black-culture  2009  beyonce-knowles      Pop   \n",
       "...        ...                          ...   ...              ...      ...   \n",
       "362232  362232    who-am-i-drinking-tonight  2012       edens-edge  Country   \n",
       "362233  362233                         liar  2012       edens-edge  Country   \n",
       "362234  362234                  last-supper  2012       edens-edge  Country   \n",
       "362235  362235  christ-alone-live-in-studio  2012       edens-edge  Country   \n",
       "362236  362236                         amen  2012       edens-edge  Country   \n",
       "\n",
       "                                                   lyrics  \n",
       "0       Oh baby, how you doing?\\nYou know I'm gonna cu...  \n",
       "1       playin' everything so easy,\\nit's like you see...  \n",
       "2       If you search\\nFor tenderness\\nIt isn't hard t...  \n",
       "3       Oh oh oh I, oh oh oh I\\n[Verse 1:]\\nIf I wrote...  \n",
       "4       Party the people, the people the party it's po...  \n",
       "...                                                   ...  \n",
       "362232  I gotta say\\nBoy, after only just a couple of ...  \n",
       "362233  I helped you find her diamond ring\\nYou made m...  \n",
       "362234  Look at the couple in the corner booth\\nLooks ...  \n",
       "362235  When I fly off this mortal earth\\nAnd I'm meas...  \n",
       "362236  I heard from a friend of a friend of a friend ...  \n",
       "\n",
       "[332423 rows x 6 columns]"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the data\n",
    "data = raw_data\n",
    "print(data.shape)\n",
    "data = data.loc[data[\"lyrics\"].str.len() > 3] # filter for songs with more than 3 words\n",
    "data = raw_data[raw_data[\"genre\"] != \"Not Available\"] # remove \"Not Available genre\"\n",
    "data = data[data[\"genre\"].notnull()] # remove null genres\n",
    "print(data.shape)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suffle the data\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "data = shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rock          131377\n",
       "Pop            49444\n",
       "Hip-Hop        33965\n",
       "Metal          28408\n",
       "Other          23683\n",
       "Country        17286\n",
       "Jazz           17147\n",
       "Electronic     16205\n",
       "R&B             5935\n",
       "Indie           5732\n",
       "Folk            3241\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres_distribution = data['genre'].value_counts()\n",
    "genres_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x13d845d30>"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "genres_distribution.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Songs with no lyrics is 95680 / 362237 = 0.264136463144295\n",
      "Instrumental songs is 80 / 362237 0.00022084988557215303%\n"
     ]
    }
   ],
   "source": [
    "data_missing_lyrics = raw_data[raw_data['lyrics'].isnull()]\n",
    "print(f\"Songs with no lyrics is {len(data_missing_lyrics)} / {len(raw_data['lyrics'])} = {len(data_missing_lyrics) / len(raw_data['lyrics'])}\")\n",
    "      \n",
    "data_instrumental_lyrics = raw_data[raw_data['lyrics'] == \"instrumental\"]\n",
    "print(f\"Instrumental songs is {len(data_instrumental_lyrics)} / {len(raw_data['lyrics'])} {len(data_instrumental_lyrics) / len(raw_data['lyrics'])}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['lyrics'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['lyrics'] != \"Not Available\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 10:27:14: 'pattern' package not found; tag filters are not available for English\n",
      "INFO - 10:27:14: loading Word2Vec object from 300features_40minwords_10context.wv.model\n",
      "INFO - 10:27:15: loading wv recursively from 300features_40minwords_10context.wv.model.wv.* with mmap=None\n",
      "INFO - 10:27:15: setting ignored attribute vectors_norm to None\n",
      "INFO - 10:27:15: loading vocabulary recursively from 300features_40minwords_10context.wv.model.vocabulary.* with mmap=None\n",
      "INFO - 10:27:15: loading trainables recursively from 300features_40minwords_10context.wv.model.trainables.* with mmap=None\n",
      "INFO - 10:27:15: setting ignored attribute cum_table to None\n",
      "INFO - 10:27:15: loaded 300features_40minwords_10context.wv.model\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_model = Word2Vec.load(\"300features_40minwords_10context.wv.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "PAD = \"<pad>\"\n",
    "w2v_model.wv[\"<pad>\"] = np.zeros(300,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def process_document(text, remove_panctuations=True, \n",
    "                        remove_stopwords=True, remove_unknown_words=True, fix_length=200):\n",
    "    \n",
    "    if remove_panctuations:\n",
    "      text = re.sub(\"[^a-zA-Z0-9]\",\" \", text)\n",
    "    #\n",
    "    # Convert words to lower case and split them\n",
    "    words = text.lower().split()\n",
    "    #\n",
    "    # Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w.lower() in stops]  \n",
    "        \n",
    "    if remove_unknown_words:\n",
    "        words = [w for w in words if w in w2v_model.wv]\n",
    "        \n",
    "    if len(words) < fix_length:\n",
    "        words += ([PAD] * (fix_length - len(words)))\n",
    "    elif len(words) > fix_length:\n",
    "        words = words[:fix_length]\n",
    "        \n",
    "#     processed_document = \" \".join(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text): # create a tokenizer function\n",
    "    return process_document(text) #[tok.text for tok in spacy_en.tokenizer(process_document(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 10,  7, ..., 10, 10, 10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "ohe = LabelBinarizer()\n",
    "ohe.fit_transform(data['genre'].values)\n",
    "\n",
    "def one_hot_encoding(label):\n",
    "    vec = ohe.transform([label])[0]\n",
    "    return vec\n",
    "\n",
    "\n",
    "\n",
    "# Import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# creating labelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# Converting string labels into numbers.\n",
    "genres_encoded = le.fit_transform(data[\"genre\"])\n",
    "genres_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SongsDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pandas.DataFrame): DataFrame object\n",
    "        \"\"\"\n",
    "        self.texts = df['lyrics'].values\n",
    "        self.labels = df['genre'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        text = tokenizer(text)\n",
    "        text = [w2v_model.wv.vocab[word].index for word in text]\n",
    "        label = self.labels[idx]\n",
    "        label = one_hot_encoding(label)\n",
    "\n",
    "        sample = (torch.tensor(text), torch.tensor(label))\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1328,  1797,   301,  4398,  1060,    97,  1172,  4861,  5370, 16835,\n",
       "          1209,   429,  3530,   989,  5701,  2115,  1029,   147,  1029,   147,\n",
       "          1029,   147,  1029,   147,   490,  3909,  1165,  2762,  1473,   721,\n",
       "           194,   564,  4234,  1316,  2355,  4790,  1951,   399,  5940,  6893,\n",
       "          1029,   147,  1029,   147,  1029,   147,  1029,   147,  1914,   615,\n",
       "          1199,   222,  1968,    52,  1029,   147,  1029,   147,  1029,   147,\n",
       "          1029,   147,  1914,   615,  1199,   222,  1968,    52,  1199,   222,\n",
       "          1914,   615,   212,    52,   135,   120,   964,  1323,  1033,  1522,\n",
       "            77,  1360,  3985,    81,  4585,  2610,    65,  2610,    50,    65,\n",
       "            55,    30,   681,  1627,  1202,  1050,   612,    70,   423,   400,\n",
       "           253,  2812,  2330,    72,  4341,   135,  3608,   181,  7154,   882,\n",
       "          1827,   825,    52,    65,    70,   440,    77,  1285,   368,   500,\n",
       "           151,   748, 17993,   619,   612,    70,   423, 29972, 29972, 29972,\n",
       "         29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972,\n",
       "         29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972,\n",
       "         29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972,\n",
       "         29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972,\n",
       "         29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972,\n",
       "         29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972,\n",
       "         29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972, 29972]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = SongsDataset(data[:1000])\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_data, val_data = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=8, shuffle=True)\n",
    "\n",
    "dataset.__getitem__(99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights, freeze_embeddings=False):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "        output_size : 2 = (pos, neg)\n",
    "        hidden_sie : Size of the hidden_state of the LSTM\n",
    "        vocab_size : Size of the vocabulary containing unique words\n",
    "        embedding_length : Embeddding dimension of GloVe word embeddings\n",
    "        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_length = embedding_length\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(weights), freeze=freeze_embeddings)\n",
    "#         self.word_embeddings = nn.Embedding(vocab_size, embedding_length)# Initializing the look-up table.\n",
    "#         self.word_embeddings.weight = nn.Parameter(weights, requires_grad=False) # Assigning the look-up table to the pre-trained GloVe word embedding.\n",
    "        self.lstm = nn.LSTM(embedding_length, hidden_size)\n",
    "        self.dense = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_sentence, batch_size=None):\n",
    "\n",
    "        \"\"\" \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
    "        batch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Output of the linear layer containing logits for positive & negative class which receives its input as the final_hidden_state of the LSTM\n",
    "        final_output.shape = (batch_size, output_size)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ''' Here we will map all the indexes present in the input sequence to the corresponding word vector using our pre-trained word_embedddins.'''\n",
    "        input = self.word_embeddings(input_sentence) # embedded input of shape = (batch_size, num_sequences,  embedding_length)\n",
    "        input = input.permute(1, 0, 2) # input.size() = (num_sequences, batch_size, embedding_length)\n",
    "\n",
    "        if batch_size is None:\n",
    "            h_0 = torch.zeros(1, self.batch_size, self.hidden_size) # Initial hidden state of the LSTM\n",
    "            c_0 = torch.zeros(1, self.batch_size, self.hidden_size) # Initial cell state of the LSTM\n",
    "        else:\n",
    "            h_0 = torch.zeros(1, batch_size, self.hidden_size)\n",
    "            c_0 = torch.zeros(1, batch_size, self.hidden_size)\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n",
    "        logits = self.dense(final_hidden_state[-1]) # final_hidden_state.size() = (1, batch_size, hidden_size) & final_output.size() = (batch_size, output_size)\n",
    "    \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 11 # 11 genres of songs\n",
    "DROPOUT = 0.25\n",
    "VOCAB_SIZE = len(w2v_model.wv.vocab)\n",
    "EMBED_DIM = 300\n",
    "HIDDEN_SIZE=256\n",
    "BATCH_SIZE=8\n",
    "weights =  w2v_model.wv.vectors #TEXT.vocab.vectors\n",
    "\n",
    "model = LSTMClassifier(batch_size=BATCH_SIZE, output_size=NUM_CLASSES, hidden_size=HIDDEN_SIZE, \n",
    "                       vocab_size=VOCAB_SIZE, embedding_length=EMBED_DIM, weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    \n",
    "    probs = torch.softmax(preds, dim=1)\n",
    "    winners = probs.argmax(dim=1)\n",
    "    correct = (winners == y.argmax(dim=1)).float() #convert into float for division \n",
    "\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for index, batch in enumerate(loader):\n",
    "        \n",
    "        # handle case where one of the batches is of size that is smaller than declared batch size\n",
    "        # todo!!!\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch[0]).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch[1])\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch[1])\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for index, batch in enumerate(loader):\n",
    "\n",
    "            predictions = model(batch[0]).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch[1])\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch[1])\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)\n",
    "\n",
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 27s\n",
      "\tTrain Loss: 0.289 | Train Acc: 40.25%\n",
      "\t Val. Loss: 0.258 |  Val. Acc: 38.00%\n",
      "Epoch: 02 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.244 | Train Acc: 43.75%\n",
      "\t Val. Loss: 0.258 |  Val. Acc: 38.00%\n",
      "Epoch: 03 | Epoch Time: 0m 24s\n",
      "\tTrain Loss: 0.238 | Train Acc: 44.75%\n",
      "\t Val. Loss: 0.248 |  Val. Acc: 43.00%\n",
      "Epoch: 04 | Epoch Time: 0m 24s\n",
      "\tTrain Loss: 0.229 | Train Acc: 49.12%\n",
      "\t Val. Loss: 0.250 |  Val. Acc: 43.00%\n",
      "Epoch: 05 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.224 | Train Acc: 49.75%\n",
      "\t Val. Loss: 0.247 |  Val. Acc: 41.50%\n",
      "Epoch: 06 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.220 | Train Acc: 50.88%\n",
      "\t Val. Loss: 0.249 |  Val. Acc: 41.00%\n",
      "Epoch: 07 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.218 | Train Acc: 51.75%\n",
      "\t Val. Loss: 0.244 |  Val. Acc: 39.50%\n",
      "Epoch: 08 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.215 | Train Acc: 52.12%\n",
      "\t Val. Loss: 0.249 |  Val. Acc: 43.50%\n",
      "Epoch: 09 | Epoch Time: 0m 21s\n",
      "\tTrain Loss: 0.214 | Train Acc: 52.75%\n",
      "\t Val. Loss: 0.251 |  Val. Acc: 40.50%\n",
      "Epoch: 10 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.212 | Train Acc: 53.25%\n",
      "\t Val. Loss: 0.246 |  Val. Acc: 40.50%\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "all_train_losses = []\n",
    "all_val_losses = []\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    all_train_losses.append(train_loss)\n",
    "    train_accuracy.append(train_acc)\n",
    "\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    all_val_losses.append(val_loss)\n",
    "    val_accuracy.append(val_acc)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best-model.model')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. Acc: {val_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_train_losses, 'r')\n",
    "plt.plot(all_val_losses, 'g')\n",
    "plt.plot(train_accuracy, 'b')\n",
    "plt.plot(val_accuracy, 'y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN for Text Generation\n",
    "In this section, we'll use an LSTM to generate new songs. You can pick any genre you like, or just use all genres. You can even try to generate songs in the style of a certain artist - remember that the Metrolyrics dataset contains the author of each song. \n",
    "\n",
    "For this, we’ll first train a character-based language model. We’ve mostly discussed in class the usage of RNNs to predict the next word given past words, but as we’ve mentioned in class, RNNs can also be used to learn sequences of characters.\n",
    "\n",
    "First, please go through the [PyTorch tutorial](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html) on generating family names. You can download a .py file or a jupyter notebook with the entire code of the tutorial. \n",
    "\n",
    "As a reminder of topics we've discussed in class, see Andrej Karpathy's popular blog post [\"The Unreasonable Effectiveness of Recurrent Neural Networks\"](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). You are also encouraged to view [this](https://gist.github.com/karpathy/d4dee566867f8291f086) vanilla implementation of a character-level RNN, written in numpy with just 100 lines of code, including the forward and backward passes.  \n",
    "\n",
    "Other tutorials that might prove useful:\n",
    "1. http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/\n",
    "1. https://github.com/mcleonard/pytorch-charRNN\n",
    "1. https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from csv\n",
    "import pandas as pd\n",
    "\n",
    "raw_data = pd.read_csv(\"380000-lyrics-from-metrolyrics.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   lyrics    genre\n",
      "0       Oh baby, how you doing?\\nYou know I'm gonna cu...      Pop\n",
      "1       playin' everything so easy,\\nit's like you see...      Pop\n",
      "2       If you search\\nFor tenderness\\nIt isn't hard t...      Pop\n",
      "3       Oh oh oh I, oh oh oh I\\n[Verse 1:]\\nIf I wrote...      Pop\n",
      "4       Party the people, the people the party it's po...      Pop\n",
      "...                                                   ...      ...\n",
      "362232  I gotta say\\nBoy, after only just a couple of ...  Country\n",
      "362233  I helped you find her diamond ring\\nYou made m...  Country\n",
      "362234  Look at the couple in the corner booth\\nLooks ...  Country\n",
      "362235  When I fly off this mortal earth\\nAnd I'm meas...  Country\n",
      "362236  I heard from a friend of a friend of a friend ...  Country\n",
      "\n",
      "[362237 rows x 2 columns]\n",
      "                                                   lyrics genre\n",
      "0       Oh baby, how you doing?\\nYou know I'm gonna cu...   Pop\n",
      "1       playin' everything so easy,\\nit's like you see...   Pop\n",
      "2       If you search\\nFor tenderness\\nIt isn't hard t...   Pop\n",
      "3       Oh oh oh I, oh oh oh I\\n[Verse 1:]\\nIf I wrote...   Pop\n",
      "4       Party the people, the people the party it's po...   Pop\n",
      "...                                                   ...   ...\n",
      "362210  When the photographs you're taking now\\nAre ta...   Pop\n",
      "362211  I met Moko jumbie,\\nHe walks on stilts through...   Pop\n",
      "362212  Chill on the hollow ponds\\nSet sail by a kid\\n...   Pop\n",
      "362213  Celebrate the passing drugs\\nPut them on the b...   Pop\n",
      "362214  When the serve is done\\nAnd the parish shuffle...   Pop\n",
      "\n",
      "[49444 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# let's see the data: browse through different songe genres:\n",
    "print(raw_data[['lyrics', 'genre']])\n",
    "\n",
    "print(raw_data[raw_data['genre'] == 'Pop'][['lyrics', 'genre']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362237"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_data.dropna(subset=['genre']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362237"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x138028e10>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "genres_distribution = data['genre'].value_counts()\n",
    "genres_distribution.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         384\n",
       "1         220\n",
       "2         116\n",
       "3         436\n",
       "4         280\n",
       "         ... \n",
       "362232    280\n",
       "362233    175\n",
       "362234    213\n",
       "362235    145\n",
       "362236    287\n",
       "Name: lyrics, Length: 266556, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(raw_data.dropna())['lyrics'].apply(lambda lyrics: len(lyrics.split(\" \")) if lyrics is not None else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word level generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 10:35:49: loading Word2Vec object from 300features_40minwords_10context.wv.model\n",
      "INFO - 10:35:50: loading wv recursively from 300features_40minwords_10context.wv.model.wv.* with mmap=None\n",
      "INFO - 10:35:50: setting ignored attribute vectors_norm to None\n",
      "INFO - 10:35:50: loading vocabulary recursively from 300features_40minwords_10context.wv.model.vocabulary.* with mmap=None\n",
      "INFO - 10:35:50: loading trainables recursively from 300features_40minwords_10context.wv.model.trainables.* with mmap=None\n",
      "INFO - 10:35:50: setting ignored attribute cum_table to None\n",
      "INFO - 10:35:50: loaded 300features_40minwords_10context.wv.model\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# load the w2v model\n",
    "w2v_model = Word2Vec.load(\"300features_40minwords_10context.wv.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=16\n",
    "VOCAB_SIZE = len(w2v_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(text, remove_panctuations=True, \n",
    "                        remove_stopwords=True, remove_unknown_words=True, fix_length=200):\n",
    "    \n",
    "    if remove_panctuations:\n",
    "      text = re.sub(\"[^a-zA-Z0-9]\",\" \", text)\n",
    "    #\n",
    "    # Convert words to lower case and split them\n",
    "    words = text.lower().split()\n",
    "    #\n",
    "    # Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w.lower() in stops]  \n",
    "        \n",
    "    if remove_unknown_words:\n",
    "        words = [w for w in words if w in w2v_model.wv]\n",
    "        \n",
    "    if len(words) < fix_length:\n",
    "        words += ([PAD] * (fix_length - len(words)))\n",
    "    elif len(words) > fix_length:\n",
    "        words = words[:fix_length]\n",
    "        \n",
    "#     processed_document = \" \".join(words)\n",
    "    return words\n",
    "\n",
    "def tokenizer(text, **kwargs): # create a tokenizer function\n",
    "    return process_document(text, **kwargs) #[tok.text for tok in spacy_en.tokenizer(process_document(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class SongsGenerationDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, df, batch_size, vocab_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pandas.DataFrame): DataFrame object\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        lyrics = df['lyrics'].values\n",
    "        lyrics = [tokenizer(text, fix_length=-1) for text in lyrics]\n",
    "        lyrics = [word for words in lyrics for word in words] # flatten list\n",
    "        # Next step, we will convert word tokens into integer indices. \n",
    "        # These will be the input to the network. And because we will train a mini-batch each iteration, \n",
    "        # we should be able to split the data into batches evenly. \n",
    "        # We can assure that by chopping out the last uneven batch\n",
    "        lyrics = [w2v_model.wv.vocab[word].index for word in lyrics]\n",
    "        batches_number = (int)(len(lyrics) / batch_size)\n",
    "        lyrics = lyrics[:(batch_size * batches_number)]\n",
    "        print(len(lyrics))\n",
    "        print(batches_number)\n",
    "        print(batch_size)\n",
    "        \n",
    "        self.words_in = lyrics\n",
    "        self.words_out = np.zeros_like(self.words_in)\n",
    "        self.words_out[:-1] = self.words_in[1:]\n",
    "        self.words_out[-1] = self.words_in[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words_in)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.words_in[idx]\n",
    "#         label = np.zeros(self.vocab_size)\n",
    "#         label[self.words_out[idx]] = 1\n",
    "        label = self.words_out[idx]\n",
    "\n",
    "        sample = (torch.tensor(text), torch.tensor(label))\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87232\n",
      "5452\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "dataset = SongsGenerationDataset(data[data[\"genre\"] == \"Country\"][:1000], BATCH_SIZE, VOCAB_SIZE)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_data, val_data = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(304), tensor(525))"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM2(nn.Module):\n",
    "    def __init__(self, n_vocab, embedding_size, hidden_size, weights, batch_size, freeze_embeddings=True):\n",
    "        super(LSTM2, self).__init__()\n",
    "    \n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = 1\n",
    "        \n",
    "        if weights is not None:\n",
    "            self.word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(weights), freeze=freeze_embeddings)\n",
    "        else:\n",
    "            self.word_embeddings = nn.Embedding(n_vocab, embedding_size)\n",
    "            \n",
    "        self.lstm = nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=self.num_layers)\n",
    "    \n",
    "        self.dense = nn.Linear(hidden_size, n_vocab)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.word_embeddings(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        output = self.dense(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, state\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_size),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a model\n",
    "VOCAB_SIZE = len(w2v_model.wv.vocab)\n",
    "EMBED_DIM = 300\n",
    "HIDDEN_SIZE =32\n",
    "weights =  w2v_model.wv.vectors #TEXT.vocab.vectors\n",
    "\n",
    "model = LSTM2(n_vocab=VOCAB_SIZE, embedding_size=EMBED_DIM, hidden_size=HIDDEN_SIZE, batch_size=BATCH_SIZE, weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)\n",
    "        \n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    \n",
    "    winners = preds.argmax(dim=1)\n",
    "    correct = (winners == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    state_h, state_c = model.init_hidden()\n",
    "    \n",
    "    for index, (x, y) in enumerate(loader):\n",
    "        x = x.unsqueeze(0)\n",
    "        y = torch.autograd.Variable(y).long()\n",
    "        \n",
    "        if x.shape[1] != BATCH_SIZE:\n",
    "            print(\"x.shape\", x.shape)\n",
    "            continue\n",
    "        else:\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            predictions, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            predictions = torch.squeeze(predictions, 0)\n",
    "        \n",
    "            loss = criterion(predictions, y)\n",
    "            acc = binary_accuracy(predictions, y)\n",
    "        \n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "        \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    state_h, state_c = model.init_hidden()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for index, (x, y) in enumerate(loader):\n",
    "            x = x.unsqueeze(0)\n",
    "            \n",
    "            if x.shape[1] != BATCH_SIZE:\n",
    "                print(\"x.shape\", x.shape)\n",
    "                continue\n",
    "                \n",
    "            predictions, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            predictions = torch.squeeze(predictions, 0)\n",
    "            \n",
    "            loss = criterion(predictions, y)\n",
    "            acc = binary_accuracy(predictions, y)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "N_EPOCHS = 5\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "all_train_losses = []\n",
    "all_val_losses = []\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    all_train_losses.append(train_loss)\n",
    "    train_accuracy.append(train_acc)\n",
    "\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    all_val_losses.append(val_loss)\n",
    "    val_accuracy.append(val_acc)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'lstm-gen-best-model.model')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. Acc: {val_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300features_40minwords_10context.wv.model\r\n",
      "380000-lyrics-from-metrolyrics.zip\r\n",
      "DL_rnn_text_classification_generation.ipynb\r\n",
      "DL_word_embedding_assignment.ipynb\r\n",
      "README.md\r\n",
      "best-model.model\r\n",
      "best-model.pt\r\n",
      "cnn.py\r\n",
      "environment.yml\r\n",
      "lstm-gen-best-model.model\r\n",
      "lstm-gen-best-model.pt\r\n",
      "\u001b[1m\u001b[36mproject-tv-script-generation\u001b[m\u001b[m\r\n",
      "test.csv\r\n",
      "train.csv\r\n",
      "val.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, model, length, top_k=5):\n",
    "    model.eval()\n",
    "    words = [w]\n",
    "    state_h, state_c = (torch.zeros(1, 1, 32), torch.zeros(1, 1, 32))\n",
    "    \n",
    "    for _ in range(length):\n",
    "        x = torch.tensor([w2v_model.wv.vocab[w].index])\n",
    "        x = x.unsqueeze(0)\n",
    "        output, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "    \n",
    "        _, top_index = torch.topk(output[0], k=top_k)\n",
    "        choices = top_index.tolist()\n",
    "        choice = np.random.choice(choices[0])\n",
    "        w = w2v_model.wv.index2word[choice]\n",
    "        words.append(w) #w2v_model.wv.vocab[choice])\n",
    "\n",
    "    print(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog i the and i the i and and the to i i and and i the the you to you i to and to to the the you you you to you and the and to you i to i and you and to and and the the i the\n"
     ]
    }
   ],
   "source": [
    "predict('dog', model, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char level generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" \\n\" #\" .,;'-\\n\"\n",
    "n_letters = len(all_letters) + 1 # Plus EOSong marker\n",
    "print(n_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_letters[len(all_letters) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    \"\"\"\n",
    "    Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "    \"\"\"\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "def readLyrics(lyrics):\n",
    "    return unicodeToAscii(lyrics)\n",
    "\n",
    "genre_lyrics = {}\n",
    "all_genres = []\n",
    "\n",
    "for _, song in data[data[\"genre\"] == \"Country\"][:1000].iterrows():\n",
    "    genre = song[\"genre\"]\n",
    "    all_genres.append(genre)\n",
    "    lyrics = readLyrics(song[\"lyrics\"])\n",
    "    genre_lyrics[genre] = lyrics\n",
    "\n",
    "n_genres = len(all_genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "            \n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        batch_size = input.size(0)\n",
    "        encoded = self.encoder(input)\n",
    "        output, hidden = self.lstm(encoded, hidden)\n",
    "        output = self.decoder(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.n_layers, batch_size, self.hidden_size),\n",
    "                torch.zeros(self.n_layers, batch_size, self.hidden_size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "#### Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot vector for category\n",
    "def genre_tensor(genre):\n",
    "    idx = all_genres.index(genre)\n",
    "    tensor = torch.zeros(1, n_genres)\n",
    "    tensor[0][idx] = 1\n",
    "    return tensor\n",
    "\n",
    "# One-hot matrix of first to last letters (not including EOS) for input\n",
    "def input_tensor(lyrics):\n",
    "    tensor = torch.zeros([len(lyrics), 1, n_letters], dtype=torch.long)\n",
    "    for idx in range(len(lyrics)):\n",
    "        letter = lyrics[idx]\n",
    "        tensor[idx][0][all_letters.find(letter)] = 1\n",
    "    print(tensor)\n",
    "    return tensor\n",
    "\n",
    "# LongTensor of second letter to end (EOS) for target\n",
    "def target_tensor(lyrics):\n",
    "    letter_indexes = [all_letters.find(lyrics[idx]) for idx in range(1, len(lyrics))] # begin from index 1\n",
    "    letter_indexes.append(n_letters - 1) # EOSong\n",
    "    return torch.LongTensor(letter_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Random item from a list\n",
    "def random_choice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "# Get a random category and random line from that category\n",
    "def random_training_pair():\n",
    "    genre = random_choice(all_genres)\n",
    "    lyrics = random_choice(genre_lyrics[genre])\n",
    "    return genre, lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_training_example():\n",
    "    genre, lyrics = random_training_pair()\n",
    "    input_genre_tensor = genre_tensor(genre)\n",
    "    input_lyrics_tensor = input_tensor(lyrics)\n",
    "    target_lyrics_tensor = target_tensor(lyrics)\n",
    "    return genre_tensor, input_lyrics_tensor, target_lyrics_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 52, 11, 14, 21, 4, 52, 1, 0, 13, 0, 13, 0, 53]\n",
      "[52, 11, 14, 21, 4, 52, 1, 0, 13, 0, 13, 0, 53, 54]\n"
     ]
    }
   ],
   "source": [
    "str = 'i love banana\\n'\n",
    "print([all_letters.find(str[idx]) for idx in range(0, len(str))] )\n",
    "print([all_letters.find(str[idx]) for idx in range(1, len(str))] + [n_letters - 1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class SongsCharactersDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, songs, batch_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            songs (pandas.DataFrame): DataFrame object\n",
    "        \"\"\"\n",
    "        self.input = []\n",
    "        self.target = []\n",
    "        \n",
    "        for lyrics in songs['lyrics']:\n",
    "            lyrics = lyrics.lower().strip()\n",
    "            lyrics = re.sub(\"[^a-zA-Z\\s\\n]\",\" \", lyrics)\n",
    "            self.input += [all_letters.find(lyrics[idx]) for idx in range(0, len(lyrics))] \n",
    "            self.target += [all_letters.find(lyrics[idx]) for idx in range(1, len(lyrics))] # begin from index 1\n",
    "            self.target += [n_letters - 1] # EOSong\n",
    "            \n",
    "        batches_number = (int)(len(self.input) / batch_size)\n",
    "        limit = (batch_size * batches_number)\n",
    "        self.input = self.input[:limit]\n",
    "        self.target = self.target[:limit]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        in_c = self.input[idx]\n",
    "        out_c = self.target[idx]\n",
    "        item = (torch.tensor(in_c), torch.tensor(out_c))\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)\n",
    "        \n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    \n",
    "    winners = preds.argmax(dim=1)\n",
    "    correct = (winners == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    state_h, state_c = model.init_hidden(BATCH_SIZE)\n",
    "    \n",
    "    for index, (x, y) in enumerate(loader):\n",
    "        x = x.unsqueeze(0)\n",
    "        y = torch.autograd.Variable(y).long()\n",
    "        \n",
    "        if x.shape[1] != BATCH_SIZE:\n",
    "            continue\n",
    "        else:\n",
    "            optimizer.zero_grad()\n",
    "            predictions, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            predictions = torch.squeeze(predictions, 0)\n",
    "            \n",
    "            loss = criterion(predictions, y)\n",
    "            acc = binary_accuracy(predictions, y)\n",
    "        \n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "        \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    state_h, state_c = model.init_hidden(BATCH_SIZE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for index, (x, y) in enumerate(loader):\n",
    "            x = x.unsqueeze(0)\n",
    "            \n",
    "            if x.shape[1] != BATCH_SIZE:\n",
    "                continue\n",
    "                \n",
    "            predictions, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            predictions = torch.squeeze(predictions, 0)\n",
    "            \n",
    "            loss = criterion(predictions, y)\n",
    "            acc = binary_accuracy(predictions, y)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "model = CharLSTM(input_size=n_letters, hidden_size=128, output_size=n_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SongsCharactersDataset(data[data[\"genre\"] == \"Country\"][:10], BATCH_SIZE)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_data, val_data = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(52), tensor(2))"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 2.710 | Train Acc: 22.95%\n",
      "\t Val. Loss: 2.449 |  Val. Acc: 25.87%\n",
      "Epoch: 02 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 2.415 | Train Acc: 25.64%\n",
      "\t Val. Loss: 2.384 |  Val. Acc: 25.75%\n",
      "Epoch: 03 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 2.378 | Train Acc: 26.02%\n",
      "\t Val. Loss: 2.373 |  Val. Acc: 25.93%\n",
      "Epoch: 04 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 2.358 | Train Acc: 26.06%\n",
      "\t Val. Loss: 2.358 |  Val. Acc: 23.84%\n",
      "Epoch: 05 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 2.349 | Train Acc: 25.73%\n",
      "\t Val. Loss: 2.369 |  Val. Acc: 25.52%\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "N_EPOCHS = 5\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "all_train_losses = []\n",
    "all_val_losses = []\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    all_train_losses.append(train_loss)\n",
    "    train_accuracy.append(train_acc)\n",
    "\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    all_val_losses.append(val_loss)\n",
    "    val_accuracy.append(val_acc)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'lstm-gen-best-model.model')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. Acc: {val_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAWLUlEQVR4nO3dfYwcd33H8c93Z3bPB45jW74mJvFDoJECAaVJT4mjlGIgKA8k2H8gCFJB4p9ABIgAVUNBCuKPSuEf1AIVUSgRScuDUEG2MQ4hhUACgcDZTUIck8pNiHJNZJvYOdvJPezOfvvHzHr31vt4t3dz9/P7JY1m5je/mf3659vP7M7tzpm7CwCw/BXyLgAAMBgEOgAEgkAHgEAQ6AAQCAIdAAIR5/XA69at882bN+f18ACwLO3du/fP7j7Saltugb5582aNjY3l9fAAsCyZ2XPttnHJBQACQaADQCAIdAAIBIEOAIEg0AEgEAQ6AASCQAeAQCy/QH/2Wenzn5d+9zupWs27GgBYMpZfoD/6qPSlL0lXXCFt2CDdcov0k59I09N5VwYAuVp+gX7TTdLhw9K990pXXin9+79L110njYxI73uf9J3vSC+/nHeVALDoLK+/WDQ6OuoD+er/1JT0s59JO3dKu3ZJhw5JcSy97W3Stm3ptHHj/B8HAJYAM9vr7qMtty37QG9UraaXZHbuTKc//jFtv/TSerhfcolkNtjHBYBFcuYEerOnn66H+29+I7lLmzbVw/2tb5WKxYWtAQAG6MwN9EaHDkm7d6fh/sAD6aWaNWukd787DfdrrpHOOmvx6gGAOSDQm73yivTTn6bhvnu39NJLUqkkvfOd0vbt0o03SuvX51MbAHRAoHdSqUi//nX90swzz6TtV1yRvnLfvl266CKuuwNYEgj0XrlL+/dLO3ak4V6r78IL69fdr7xSiqJ86wRwxiLQ52p8XPrRj9KAf/BBqVxOP+9+441puL/rXdLwcN5VAjiDEOiDMDGRfiN1507pxz+Wjh9Pw/yaa9Jwv+EGad26vKsEEDgCfdBmZqRf/rJ+3X18XCoUpKuuSq+5b9smveENeVcJIEAE+kJyl/btq4f7E0+k7RdfXL/uPjqaBj4AzBOBvpiefbYe7g8/LCWJ9LrXSe95Txrub3+7NDSUd5UAlqlOgd71ZaOZbTCzB83sgJntN7NPtuiz1cwmzOyxbLp9EIUvSxdcIN16a/pL1EOHpHvukbZsmX0Tsfe/n5uIARi4rq/QzWy9pPXuvs/MzpK0V9J2d3+qoc9WSX/v7jf0+sDBvkJvp3YTsR070k/ONN5EbPv29BU8NxED0MW8XqG7+4vuvi9bPiHpgKTzBlviGWDFivQ2A9/4hvTCC9Ijj0if+Uz6C9VPfCK9x8xll0lf/KL0+OPptXkA6ENf19DNbLOkhyS92d2PN7RvlfQDSeOSXlD6an1/i/1vlnSzJG3cuPGvn3vuuXmUHhBuIgagRwP5paiZrZT0S0n/5O4/bNq2SlLV3U+a2fWS/sXdL+x0vDPukkuvDh1KL8nUbiI2Pc1NxACcMu9AN7OipN2S7nf3L/fQ/0+SRt39z+36EOg9OHly9k3Ejh5NbyJ29dVpuHMTMeCMM69ANzOTdI+ko+5+a5s+50o65O5uZpdL+k9Jm7zDweca6I88/4ju+NUdWjO8RmtXrNWa4TVas2KN1g63Xi5GgVyqqN1ErHafmWefTduvuKL+ZSZuIgYEb76B/jeSHpb0B0nVrPlzkjZKkrvfaWYfl3SLpIqkSUmfdvdHOh13roF+/8H7ddt/3aajk0d1bOqYTs6c7Nh/ZWnlrJBfO7xWa1asad3WsHz2irNVsCX6ZSB36ckn02DfsUPauzdtr91EbPv29KOS3EQMCE7QXywqJ2UdmzqmY5PHdGzqWBr0k8dOBf6xyWM6OtWibfKoppPptsc1mVavWD0r5FsF/2nbh9fotcXXyhbzlfL4ePr3VHfu5CZiQOCCDvT5mCxPtj8J1JbbbE88aXvcYqHY/lJQm8tDtW1D8Ty/RToxId13Xxrue/akNxErFqWzz5Ze85rTp+Hh1u399i2VuNwDLAICfcDcXSdmTrQ/CTQsN7dNTE90PPZwPNz6UlCL3xc0bl+9YrXiQjz7YLWbiP3852nQv/qqNDmZzjtNMzP9D0qhsHAni8aJj2/iDNcp0ONWjejMzLRqaJVWDa3SptWb+to3qSaamJ7o+K6gse2ZY8/o2Ivp8ivlVzoee9XQqpaXgla/Y7WKhTWKCusUWaSoEHWcxzJFlaqicqKoXEmnmYqimXI6ny6ny9Mz6fLUTLY8o2hqWtFkbT6laHJK8eTLil6eUvRqNk1OKXp1Mj1WVYpcp80L7V5nxPHCnCia+8Y8NbD88FO7yKJCpLXDa7V2eG3f+84kM51PAo2/R5g6pv2H9+vo5FG9PPWyKtVKx8tE8xZn02sHd8hIhWyy+txNsSeK/ISi6onsJODpiaDqipLaVFU0WVV0otr2pNE4j5va4qoUq5BOFimydD57ihUXYsWFSHEhVhTFigvFtC0qKq6tR0XFcSmbFxXFpWy9pLg4lC4XS4rjIcWlIcXxkKJiSXFpRbpeXJEt16doaFhWKqXvWGrz2lQqpSckLoH1zN1V9aqqXlXiiZJqcmre3Fb16qztvbY1Huf1a16vN468ceD/DgJ9GSlFJZ2z8hyds/KcOR+j1Q9ebV6pVtpum+t8wY/ZrX81UVItK6mk00ylrCQpK0kqaXtSSY+XVGbv61VVVFXFk3Susio2o0RVVWxAlymTbJqjQnYiaju5shNgNqkwe26R4uykGddOWIrSE1TjiasQKyrUTl7Z1HCyiqJYbqakICVyJSZVLZ0n5krkqjYsn2prWE771Jarp+bVxnWv1tuy5cRPX088ydqSU+vJqbCunvq5aXwuVL3afcAH6LarbtMdV98x8OMS6GeYghVUiAoqimvRc1V7NVepVlpOtZNOpylJyqrMTKlSnq7PTy3PqFKZVlIpp22VmfqUzKhSKWfLZSXViipJOZtqxy/Pfqxqkp6YWkxTSk9ciSrZiauqilwVq82zqZCGbUXpZ5MrLiVeW5k9Ppa90yn46ZfRWl1aa2xr3qe5rTiPY3d9PJkimQpN7wgjK6hQW7f0BFhoWK71LRSidNksvYSpggpWSJetkB4nW37dX6yRrh78zyaBDvTJak/YQqQhnaH3tneXl8tKpidVmZ6UVatp+LnJqlWpcUqSwSwv52MlTe2F1Qvy30KgA+ifmaxUUlwqKT7r7LyrQWaJfhUSANAvAh0AAkGgA0AgCHQACASBDgCBINABIBAEOgAEgkAHgEAQ6AAQCAIdAAJBoANAIAh0AAgEgQ4AgSDQASAQBDoABIJAB4BAEOgAEAgCHQACQaADQCAIdAAIBIEOAIEg0AEgEAQ6AASia6Cb2QYze9DMDpjZfjP7ZIs+ZmZfMbODZvaEmV22MOUCANqJe+hTkfQZd99nZmdJ2mtmD7j7Uw19rpN0YTZdIenr2RwAsEi6vkJ39xfdfV+2fELSAUnnNXXbJuleT/1W0mozWz/wagEAbfV1Dd3MNku6VNKjTZvOk/R8w/q4Tg99mdnNZjZmZmNHjhzpr1IAQEc9B7qZrZT0A0m3uvvx5s0tdvHTGtzvcvdRdx8dGRnpr1IAQEc9BbqZFZWG+bfd/YctuoxL2tCwfr6kF+ZfHgCgV718ysUkfVPSAXf/cptuuyR9KPu0yxZJE+7+4gDrBAB00cunXK6S9EFJfzCzx7K2z0naKEnufqekPZKul3RQ0quSPjz4UgEAnXQNdHf/lVpfI2/s45I+NqiiAAD945uiABAIAh0AAkGgA0AgCHQACASBDgCBINABIBAEOgAEgkAHgEAQ6AAQCAIdAAJBoANAIAh0AAgEgQ4AgSDQASAQBDoABIJAB4BAEOgAEAgCHQACQaADQCAIdAAIBIEOAIEg0AEgEAQ6AASCQAeAQBDoABAIAh0AAkGgA0AgCHQACASBDgCBINABIBAEOgAEomugm9ndZnbYzJ5ss32rmU2Y2WPZdPvgywQAdBP30Odbkr4m6d4OfR529xsGUhEAYE66vkJ394ckHV2EWgAA8zCoa+hXmtnjZnafmV3crpOZ3WxmY2Y2duTIkQE9NABAGkyg75O0yd0vkfRVSTvadXT3u9x91N1HR0ZGBvDQAICaeQe6ux9395PZ8h5JRTNbN+/KAAB9mXegm9m5ZmbZ8uXZMV+a73EBAP3p+ikXM/uupK2S1pnZuKQvSCpKkrvfKem9km4xs4qkSUk3ubsvWMUAgJa6Brq7f6DL9q8p/VgjACBHfFMUAAJBoANAIAh0AAgEgQ4AgSDQASAQBDoABIJAB4BAEOgAEAgCHQACQaADQCAIdAAIBIEOAIEg0AEgEAQ6AASCQAeAQBDoABAIAh0AAkGgA0AgCHQACASBDgCBINABIBAEOgAEgkAHgEAQ6AAQCAIdAAJBoANAIAh0AAgEgQ4AgSDQASAQBDoABKJroJvZ3WZ22MyebLPdzOwrZnbQzJ4ws8sGXyYAoJteXqF/S9K1HbZfJ+nCbLpZ0tfnXxYAoF9dA93dH5J0tEOXbZLu9dRvJa02s/WDKhAA0JtBXEM/T9LzDevjWRsAYBENItCtRZu37Gh2s5mNmdnYkSNHBvDQAICaQQT6uKQNDevnS3qhVUd3v8vdR919dGRkZAAPDQCoGUSg75L0oezTLlskTbj7iwM4LgCgD3G3Dmb2XUlbJa0zs3FJX5BUlCR3v1PSHknXSzoo6VVJH16oYgEA7XUNdHf/QJftLuljA6sIADAnfFMUAAJBoANAIAh0AAgEgQ4AgSDQASAQBDoABIJAB4BAEOgAEAgCHQACQaADQCAIdAAIBIEOAIEg0AEgEAQ6AASCQAeAQBDoABAIAh0AAkGgA0AgCHQACASBDgCBINABIBAEOgAEgkAHgEAQ6AAQCAIdAAJBoANAIAh0AAgEgQ4AgSDQASAQBDoABIJAB4BA9BToZnatmT1tZgfN7LMttm81swkzeyybbh98qQCATuJuHcwskvSvkt4laVzS781sl7s/1dT1YXe/YQFqBAD0oJdX6JdLOujuz7j7jKTvSdq2sGUBAPrVS6CfJ+n5hvXxrK3ZlWb2uJndZ2YXtzqQmd1sZmNmNnbkyJE5lAsAaKeXQLcWbd60vk/SJne/RNJXJe1odSB3v8vdR919dGRkpL9KAQAd9RLo45I2NKyfL+mFxg7uftzdT2bLeyQVzWzdwKoEAHTVS6D/XtKFZnaBmZUk3SRpV2MHMzvXzCxbvjw77kuDLhYA0F7XT7m4e8XMPi7pfkmRpLvdfb+ZfTTbfqek90q6xcwqkiYl3eTuzZdlAAALyPLK3dHRUR8bG8vlsQFguTKzve4+2mob3xQFgEAQ6AAQCAIdAAJBoANAIAh0AAgEgQ4AgSDQASAQBDoABIJAB4BAEOgAEAgCHQACQaADQCC63m1xqalUjmt6+v8aWmb//Y3sLr4ttrVent1/Lvv0tn9vdc1ln17//VLt75I03pCtWnW5p229zTv3qR2/n30a963X08tx632l1ttatafH6tRXSp8aJUlFuZdOLc9ui9ROt3ve9XJPvOVyjBqzdGpc7jTl0W+p1FYqSUNDvY1rP5ZdoB89er+eeup9eZeBZa7xiTZXSVJQkhRVLpdUqRRVqdTn5XLptG2NbfVt6fZB9+/U5r5c35i7CoVEcVxWHJcVRZVT8yiavT6o7YPer9Y+MXGLPvKRzw58hJZdoK9atUVvetP3JM1+pSmlr+4qFalclsrl1suVimdt9eXmfun67H2a90/7NbbPXk8SP/XKxqxeZ7vlVOttceyKYymOpWLRs3l9uXlbOrmKxXQ5ilxmlgVY87xVW7d5/Z1A++O23qdb30Khvt6qT/vHPb2tXd/6ert9XOmt/cuSZpT+bfR0uXEexzPZ9uZtMw37Nm472bb/6W0LeVvrSLV3G2b1dx3p369pXK71adVW2y9ddq9m/4Z03Nwrp9ab5+m/8fT12n6N643z9P9kcbmb0ndksdyLktJ5u/XZ8xWz1qvV+vaLLvrLBal32QX6L36xQZ/61Ps1PS1NT0tTUzq1XBnQ/3ehkL4dGhqSVqyoL7drq62vXNnffp2OVZtKJWUhhzNFegkokXtZ7jOqVtO5e1nV6kwfbfVtvbXVHq+57XibfvX+ZpHMYpkVZRYrioqz1pvnhcJrumzvtv/ct/e37/J68i27QF+9WnrLWwYfnI1t8bIbFYQkfdcQK316DuddDpaRZRddW7ZI3/9+3lUAwNKzvN5PAADaItABIBAEOgAEgkAHgEAQ6AAQCAIdAAJBoANAIAh0AAiENd8PZdEe2OyIpOfmuPs6SX8eYDmDslTrkpZubdTVH+rqT4h1bXL3kVYbcgv0+TCzMXcfzbuOZku1Lmnp1kZd/aGu/pxpdXHJBQACQaADQCCWa6DflXcBbSzVuqSlWxt19Ye6+nNG1bUsr6EDAE63XF+hAwCaEOgAEIglHehmdq2ZPW1mB83stL+oaqmvZNufMLPLlkhdW81swswey6bbF6muu83ssJk92WZ7XuPVra5FHy8z22BmD5rZATPbb2afbNFn0cerx7ryGK8VZvY7M3s8q+uLLfrkMV691JXL8zF77MjM/tvMdrfYNvjxSv9+4dKblP4l2/+V9Hqlf432cUlvaupzvaT7JJmkLZIeXSJ1bZW0O4cx+1tJl0l6ss32RR+vHuta9PGStF7SZdnyWZL+Z4n8fPVSVx7jZZJWZstFSY9K2rIExquXunJ5PmaP/WlJ32n1+AsxXkv5Ffrlkg66+zOe/tn170na1tRnm6R7PfVbSavNbP0SqCsX7v6QpKMduuQxXr3Utejc/UV335ctn5B0QNJ5Td0Wfbx6rGvRZWNwMlstZlPzJyryGK9e6sqFmZ0v6d2S/q1Nl4GP11IO9PMkPd+wPq7Tf7B76ZNHXZJ0ZfY28D4zu3iBa+pVHuPVq9zGy8w2S7pU6au7RrmOV4e6pBzGK7t88Jikw5IecPclMV491CXl8/P1z5L+QVK1zfaBj9dSDnRr0dZ85u2lz6D18pj7lN5v4RJJX5W0Y4Fr6lUe49WL3MbLzFZK+oGkW939ePPmFrssynh1qSuX8XL3xN3/StL5ki43szc3dcllvHqoa9HHy8xukHTY3fd26taibV7jtZQDfVzShob18yW9MIc+i16Xux+vvQ109z2Sima2boHr6kUe49VVXuNlZkWlofltd/9hiy65jFe3uvL++XL3lyX9QtK1TZty/flqV1dO43WVpPeY2Z+UXpZ9h5n9R1OfgY/XUg7030u60MwuMLOSpJsk7Wrqs0vSh7LfFm+RNOHuL+Zdl5mda2aWLV+udJxfWuC6epHHeHWVx3hlj/dNSQfc/cttui36ePVSV07jNWJmq7PlYUlXS/pjU7c8xqtrXXmMl7v/o7uf7+6blWbEz93975q6DXy84vnsvJDcvWJmH5d0v9JPltzt7vvN7KPZ9jsl7VH6m+KDkl6V9OElUtd7Jd1iZhVJk5Ju8uzX2gvJzL6r9Df668xsXNIXlP6SKLfx6rGuPMbrKkkflPSH7PqrJH1O0saGuvIYr17qymO81ku6x8wipYH4fXffnffzsce6cnk+trLQ48VX/wEgEEv5kgsAoA8EOgAEgkAHgEAQ6AAQCAIdAAJBoANAIAh0AAjE/wM0zTIhovR4SgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_train_losses, 'r')\n",
    "plt.plot(all_val_losses, 'g')\n",
    "plt.plot(train_accuracy, 'b')\n",
    "plt.plot(val_accuracy, 'y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 20\n",
    "\n",
    "# Sample from a category and starting letter\n",
    "def sample(start_letter='A'):\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        x = torch.tensor([all_letters.find(start_letter)])\n",
    "        x = x.unsqueeze(0)\n",
    "        print(x)\n",
    "        hidden = model.init_hidden(BATCH_SIZE)\n",
    "\n",
    "        output_name = start_letter\n",
    "\n",
    "        for i in range(max_length):\n",
    "            output, hidden = model([x], hidden)\n",
    "            topv, topi = output.topk(1)\n",
    "            topi = topi[0][0]\n",
    "            if topi == n_letters - 1:\n",
    "                break\n",
    "            else:\n",
    "                letter = all_letters[topi]\n",
    "                output_name += letter\n",
    "            input = input_tensor(letter)\n",
    "\n",
    "        return output_name\n",
    "\n",
    "# Get multiple samples from one category and multiple starting letters\n",
    "def samples(category, start_letters='ABC'):\n",
    "    for start_letter in start_letters:\n",
    "        print(sample(start_letter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict(l, model, length, top_k=5):\n",
    "    model.eval()\n",
    "    text = [l]\n",
    "    state_h, state_c = (torch.zeros(1, 1, 128), torch.zeros(1, 1, 128))\n",
    "    \n",
    "    for _ in range(length):\n",
    "        x = torch.tensor([all_letters.find(l)])\n",
    "        x = x.unsqueeze(0)\n",
    "        output, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "    \n",
    "        _, top_index = torch.topk(output[0], k=top_k)\n",
    "        choices = top_index.tolist()\n",
    "        choice = np.random.choice(choices[0])\n",
    "        l = all_letters[choice]\n",
    "        text.append(l) #w2v_model.wv.vocab[choice])\n",
    "\n",
    "    print(' '.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l i s \n",
      " t \n",
      " i s s t h   i t o n g \n",
      " a t \n",
      " w   w o n d s \n",
      " a n e t i s o n d   i   w a r e t   i n   i   s \n",
      " w e a t o n d e s \n",
      " o r   a r i n g   i n o u t   w   i f   s o r   t h o r   t   w   s   a t\n"
     ]
    }
   ],
   "source": [
    "predict('l', model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Tips\n",
    "As a final tip, I do encourage you to do most of the work first on your local machine. They say that Data Scientists spend 80% of their time cleaning the data and preparing it for training (and 20% complaining about cleaning the data and preparing it). Handling these parts on your local machine usually mean you will spend less time complaining. You can switch to the cloud once your code runs and your pipeline is in place, for the actual training using a GPU.  \n",
    "\n",
    "I also encourage you to use a small subset of the dataset first, so things run smoothly. The Metrolyrics dataset contains over 300k songs. You can start with a much much smaller set (even 3,000 songs) and try to train a network based on it. Once everything runs properly, add more data. \n",
    "\n",
    "Good luck!  \n",
    "Omri"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:biu-python] *",
   "language": "python",
   "name": "conda-env-biu-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
